{"layout":"article.njk","title":"OpenLLaMA: An Open Reproduction of LLaMA Large Language Model","date":"2023-06-07T03:00:00.000Z","formattedDate":"2023-6-7","source_link":"https://huggingface.co/openlm-research/open_llama_7b","tags":["machine-learning","natural-language-processing"],"topics":[{"name":"Machine Learning","slug":"machine-learning"},{"name":"Natural Language Processing","slug":"natural-language-processing"}],"content":"<p>OpenLLaMA is an open-source reproduction of Meta AI&#39;s LLaMA large language model, which is a pre-trained model that can generate text in a variety of styles and formats. In this repository, we provide permissively licensed PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models.</p>\n<p>We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. The models are trained on a large corpus of text data, which includes books, articles, and web pages. The models can generate text in a variety of styles, including news articles, scientific papers, and even poetry.</p>\n<p>The OpenLLaMA models are released under the Apache 2.0 license, which means that they can be used and modified by anyone. We provide the weights in two formats: an EasyLM format to be used with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.</p>\n<p>To load the weights with Hugging Face Transformers, preview checkpoints can be directly loaded from Hugging Face&#39;s model hub. The weights can be loaded using the following code snippet:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;openlm-research/open_llama_7b&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;openlm-research/open_llama_7b&quot;)\n</code></pre>\n<p>The OpenLLaMA models can be used for a variety of natural language processing tasks, including text generation, language modeling, and fine-tuning on downstream tasks. The models can be fine-tuned on a variety of tasks, including sentiment analysis, question answering, and text classification.</p>\n<p>In conclusion, OpenLLaMA is an open-source reproduction of Meta AI&#39;s LLaMA large language model, which provides pre-trained models that can generate text in a variety of styles and formats. The models are released under the Apache 2.0 license and can be used and modified by anyone. The models can be loaded using the Hugging Face transformers library, and can be fine-tuned on a variety of natural language processing tasks. For more information, please visit the project homepage of OpenLLaMA.</p>\n"}