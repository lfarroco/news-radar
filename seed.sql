--
-- PostgreSQL database dump
--

-- Dumped from database version 14.1
-- Dumped by pg_dump version 14.1

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: info; Type: TABLE; Schema: public; Owner: root
--

CREATE TABLE public.info (
    id integer NOT NULL,
    title text,
    link text,
    status character varying(32),
    source character varying(32),
    topics character varying(32),
    original text,
    date date,
    article text
);


ALTER TABLE public.info OWNER TO root;

--
-- Name: info_id_seq; Type: SEQUENCE; Schema: public; Owner: root
--

CREATE SEQUENCE public.info_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.info_id_seq OWNER TO root;

--
-- Name: info_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: root
--

ALTER SEQUENCE public.info_id_seq OWNED BY public.info.id;


--
-- Name: info id; Type: DEFAULT; Schema: public; Owner: root
--

ALTER TABLE ONLY public.info ALTER COLUMN id SET DEFAULT nextval('public.info_id_seq'::regclass);


--
-- Data for Name: info; Type: TABLE DATA; Schema: public; Owner: root
--

COPY public.info (id, title, link, status, source, topics, original, date, article) FROM stdin;
1	Stack Exchange Moderators Are Going On Strike	https://meta.stackexchange.com/questions/389811/moderation-strike-stack-overflow-inc-cannot-consistently-ignore-mistreat-an	rejected	reddit	\N		2023-06-05	\N
2	Why it is time to start thinking of games as databases	https://ajmmertens.medium.com/why-it-is-time-to-start-thinking-of-games-as-databases-e7971da33ac3	rejected	reddit	\N		2023-06-06	\N
3	Why you should gradually release features to load test your system	https://appflags.io/blog/safely-scaling-up-feature-flags-for-gradual-feature-rollouts-and-load-handling/	rejected	reddit	\N		2023-06-06	\N
4	De-stressing Booking.com	https://www.alexcharlton.co/projects/booking-com-de-stresser	rejected	reddit	\N		2023-06-06	\N
7	Any thoughts on Mojo?	https://www.modular.com/mojo	rejected	reddit	\N		2023-06-06	\N
10	Optimizing for Opportunity	https://www.developing.dev/p/optimizing-for-opportunity	rejected	reddit	\N		2023-06-06	\N
32	Match historical baseball player stats using vector search	https://huggingface.co/spaces/NeuML/baseball	failed-too-small	reddit	[]	\N	2023-06-06	\N
9	I made a boilerplate 🧪 Python/Flask + GCP Cloud Run + Docker + Gitlab · GitLab	https://gitlab.com/dag83/boilerplate-flask-gcp-cloud-run-docker-gitlab	failed-too-small	reddit	["Python","Flask","GCP Cloud Run		2023-06-06	\N
35	Service Rents Email Addresses for Account Signups	https://krebsonsecurity.com/2023/06/service-rents-email-addresses-for-account-signups/	rejected	reddit	\N	\N	2023-06-07	\N
18	Data Structures and Algorithms- an exhaustive Learning Plan	https://leapp.ai/learn/Data%20Structures%20and%20Algorithms/337e7422-6a57-403c-9449-e25ef663d3f4	failed-too-small	reddit	["Data Structures","Algorithms"]		2023-06-06	\N
12	DreamBerd: perfect programming language	https://github.com/TodePond/DreamBerd	rejected	reddit	\N		2023-06-05	\N
15	Hacking window titles to help OBS	https://blog.pkh.me/p/40-hacking-window-titles-to-help-obs.html	rejected	reddit	\N		2023-06-06	\N
16	Dear Stack Overflow, Inc.	https://openletter.mousetail.nl/	rejected	reddit	\N		2023-06-05	\N
17	How Does 5g Technology Enhance The Internet Of Things (IoT)	https://www.tvisha.com/blog/how-does-5g-technology-enhance-the-internet-of-things	rejected	reddit	\N		2023-06-06	\N
19	AI Does Not Help Programmers	https://cacm.acm.org/blogs/blog-cacm/273577-ai-does-not-help-programmers/fulltext	rejected	reddit	\N		2023-06-06	\N
20	Analyze graphs with advanced analytical methods	https://medium.com/@oieivind/analyzing-graph-networks-part-2-utilizing-advanced-methods-604ade49f9b8	rejected	reddit	\N		2023-06-06	\N
29	Letlang — Roadblocks and how to overcome them - My programming language targeting Rust	https://david-delassus.medium.com/letlang-roadblocks-and-how-to-overcome-them-38cc46c8432	failed-too-small	reddit	["Programming Language","Rust"]	\N	2023-06-07	\N
21	Modern Image Processing Algorithms Implementation in C	https://sod.pixlab.io/articles/modern-image-processing-algorithms-implementation.html	failed-too-big	reddit	["Image Processing","C"]	\N	2023-06-06	\N
22	Announcing C# Dev Kit for Visual Studio Code	https://devblogs.microsoft.com/visualstudio/announcing-csharp-dev-kit-for-visual-studio-code/	failed-too-big	reddit	["C#","Visual Studio Code"]	\N	2023-06-07	\N
23	News from WWDC23: WebKit Features in Safari 17 beta	https://webkit.org/blog/14205/news-from-wwdc23-webkit-features-in-safari-17-beta/	failed-too-big	reddit	["WebKit","Safari"]	\N	2023-06-07	\N
34	I'm working on a Visual programming language. It doesn't have a UI yet since I suck at FE development and I am learning to build what I want. But I would like some feedback on what I have so far.	https://github.com/BBloggsbott/gpvpl	skipped	reddit	[]	 BBloggsbott / gpvpl Public\nNotifications\nFork 0 Star 0 License MIT license 0 stars 0 forks Star\nNotifications Code Issues 0 Pull requests 1 Actions Projects 0 Security Insights More Code Issues Pull requests Actions Projects Security Insights BBloggsbott/gpvpl This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. development Switch branches/tags Branches Tags Could not load branches Nothing to show {{ refName }} default View all branches Could not load tags Nothing to show {{ refName }} default View all tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? Cancel Create 2 branches 0 tags Code Local Codespaces Clone HTTPS GitHub CLI Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Open with GitHub Desktop Download ZIP Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio Code Your codespace will open once ready. There was a problem preparing your codespace, please try again. Latest commit BBloggsbott Add work in progress badge … d94bd27 Jun 7, 2023 Add work in progress badge d94bd27 Git stats 23 commits Files Permalink Failed to load latest commit information. Type Name Latest commit message Commit time .github/workflows Update github action May 18, 2023 14:10 examples Add float and bool types June 3, 2023 10:59 src/main/java/org/bbloggsbott Add float and bool types June 3, 2023 10:59 .gitignore Add basic Block and Context classes May 9, 2023 20:19 LICENSE Initial commit May 9, 2023 20:10 README.md Add work in progress badge June 7, 2023 10:13 pom.xml Add basic Block and Context classes May 9, 2023 20:19 View code GPVPL - General Purpose Visual Programming Language Table of Contents Introduction A General Purpose Language Easy transition to other languages Roadmap Plug and Play Front-end README.md GPVPL - General Purpose Visual Programming Language\nTable of Contents\nTable of Contents\nIntroduction\nRoadmap\nPlug and Play Front-end\nIntroduction\nGPVPL is a Visual Programming Language built with two goals.\nA General Purpose Language\nA lot of the widely used Visual Programming Languages are either built for education (Scratch, Snap!), built to function inside specific applications (Blender's node graphs) or built for specific use cases (Cameleon, Nodal).\nGPVPL is being built with the goal to be a general purpose language that can be used for a wide variety of tasks.\nEasy transition to other languages\nSome of the Visual Programming languages built for education and giving introduction to programming are tailored to work with sprites and building graphical applications.\nGPVPL is being built with the goal to make it easier for users to transition to other widely used programming languages (like Java and python).\nThis is done by using terminology, syntax and concepts similar to these languages, making it easier for the users to correlate the concepts in those languages.\nRoadmap\nFeature / Capability\nBackend\nFrontend\nIntroduce Basic Data types - Int, String, Float, Bool\nIn Progress\nIntroduce Basic Opeators - +, -, *, /, %\nIn Progress\nIntroduce Vairable creation and assignments\nIn Progress\nIntroduce Conditional operators - >, <, >=, <=, ==, !=\nIntroduce if/else statements\nIntroduce Loops\nIntroduce Functions\nPlug and Play Front-end\nThe backend runs in two modes:\nExecutor - In this mode, the backend will execute the code. This code can be generated from the Front-end and is a JSON file.\nWeb Server - In this mode, the backend will run as a server and expose REST endpoints. The Front-end will use this API to run the Visual code.\nThis means that you are not tied to the Front-end that is available in this repo. You can build your own front end and use it with the backend to run the code. About No description, website, or topics provided. Resources Readme License MIT license\nStars 0 stars\nWatchers 2 watching\nForks 0 forks Report repository Releases No releases published Packages 0 No packages published Languages Java 100.0% LicenseBBloggsbott/gpvplName already in use Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio CodeYour codespace will open once ready.There was a problem preparing your codespace, please try again.Latest commitGit statsFiles README.md GPVPL - General Purpose Visual Programming LanguageTable of ContentsIntroductionGPVPL is a Visual Programming Language built with two goals.A General Purpose LanguageA lot of the widely used Visual Programming Languages are either built for education (Scratch, Snap!), built to function inside specific applications (Blender's node graphs) or built for specific use cases (Cameleon, Nodal).GPVPL is being built with the goal to be a general purpose language that can be used for a wide variety of tasks.Easy transition to other languagesSome of the Visual Programming languages built for education and giving introduction to programming are tailored to work with sprites and building graphical applications.GPVPL is being built with the goal to make it easier for users to transition to other widely used programming languages (like Java and python).\nThis is done by using terminology, syntax and concepts similar to these languages, making it easier for the users to correlate the concepts in those languages.RoadmapPlug and Play Front-endThe backend runs in two modes:This means that you are not tied to the Front-end that is available in this repo. You can build your own front end and use it with the backend to run the code.AboutResourcesLicenseStarsWatchersForks Releases Packages 0\nLanguagesFooterFooter navigation	2023-06-07	\N
36	Streaming HarperDB Records with NATS and Kafka	https://www.harperdb.io/post/streaming-harperdb-records-with-nats-and-kafka	published	reddit	["HarperDB","NATS","Kafka"]	Streaming HarperDB Records with NATS and KafkaTable of ContentsChange Data Capture (CDC) is a popular design pattern used to track changes in data from a source database and stream those changes to downstream processes. In HarperDB, the clustering engine and custom functions features can be used in conjunction to implement CDC. In this tutorial, we’ll see how to utilize the internal NATS streaming service and the Fastify Kafka plugin to publish new records to Kafka. ‍HarperDB SetupTo start out, we need to spin up HarperDB with custom functions enabled alongside Kafka and Zookeeper. Add the following contents to `docker-compose.yml`:‍For this example, we’ll store the database contents locally in `./harperdb` directory: `mkdir harperdb`. Also, note that we are not specifying `CLUSTERING_ENABLED=true` in docker-compose. This will break the initial startup, and we’ll configure via Harper Studio console. ‍Start up the services via `docker-compose up -d`.‍Now we need to connect our local instance to Harper Studio. Specify the username and password from the docker compose file as well as port and host. ‍‍After we log in, we can create a cluster user:‍‍Finally, let’s create a schema and table. We’ll use our favorite `dev` schema and `dog` table respectively. Custom Functions SetupHarperDB has an existing template utilizing the internal NATS stream and publishing to WebSockets: ​​https://github.com/HarperDB-Add-Ons/cf-template-websockets‍We will modify this setup to publish to Kafka. But first, clone this repo into the `custom_function` directory of your HarperDB instance. ‍```cd harperdb/custom_functionsgit clone https://github.com/HarperDB-Add-Ons/cf-template-websockets.git```‍To get this working, we need to rename `config.json.example` to `config.json` and update our NATS user and pass to one we created via HarperDB Studio. Finally, run `npm i` to install the dependencies. ‍NOTE: HarperDB Studio cannot parse file names with multiple “.” so it may say “File does not exist”. Simply rename the files if you want to see the file contents on the console. ‍Now restart HarperDB, and we can use the example client file (`client.example.js`) to test the WebSocket connection. ‍‍Once we start this function, we should see the message “open!” and adding new records to our `dog` table will print out the records:‍‍Modifying to Publish to KafkaInstead of publishing messages back to the WebSocket client, let’s now publish JSON messages to Kafka. To do so, install the Fastify Kafka library: `npm i fastify-kafkajs`.‍Then we can import and register the Kafka client. ‍‍We can now simply modify the `onPublishedMessage` function to publish to Kafka instead of writing back to the socket:‍‍Now restart the server and connect to our WebSocket client again. Publish another message to HarperDB, and we can check that it has been published to Kafka by sshing into the Kafka container and using the `kafka-console-consumer` binary:‍‍Wrapping UpIn this tutorial, we saw how to use the internal NATS stream to listen to changes to data in HarperDB. We then created a Fastify route to subscribe to those tables and publish those new messages to WebSockets and Kafka. You can modify the onPublishedMessage method to publish to multiple topics and also run this WebSocket client in the background to emulate a Debezium-like experience. Get Started With HarperDBLearning from the experts.	2023-06-06	{"title":"Using HarperDB with NATS and Kafka for Change Data Capture","article":"\\nChange Data Capture (CDC) is a popular design pattern used to track changes in data from a source database and stream those changes to downstream processes. HarperDB, a clustering engine and custom functions database, can be used to implement CDC. In this tutorial, we’ll see how to utilize the internal NATS streaming service and the Fastify Kafka plugin to publish new records to Kafka.\\n\\nHarperDB Setup\\nTo start, spin up HarperDB with custom functions enabled alongside Kafka and Zookeeper. Add the following contents to `docker-compose.yml`:\\n\\n```\\nversion: \\"3.7\\"\\n\\nservices:\\n  harperdb:\\n    image: harperdb/hdb:v2.3.3\\n    ports:\\n      - \\"9925:9925\\"\\n      - \\"9926:9926\\"\\n    volumes:\\n      - ./harperdb:/opt/hdb-data\\n      - ./harperdb/custom_functions:/opt/hdb/custom_functions\\n    environment:\\n      - HDB_LICENSE_KEY=YOUR_LICENSE_KEY\\n      - HDB_CUSTOM_FUNCTIONS_ENABLED=true\\n      - HDB_KAFKA_ENABLED=true\\n      - HDB_KAFKA_BOOTSTRAP_SERVERS=kafka:9092\\n      - HDB_KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\\n      - HDB_NATS_ENABLED=true\\n      - HDB_NATS_URL=nats://nats:4222\\n      - HDB_NATS_CLUSTER_ID=test-cluster\\n      - HDB_NATS_CLIENT_ID=test-client\\n    depends_on:\\n      - kafka\\n      - zookeeper\\n      - nats\\n\\n  kafka:\\n    image: wurstmeister/kafka\\n    ports:\\n      - \\"9092:9092\\"\\n    environment:\\n      - KAFKA_ADVERTISED_HOST_NAME=kafka\\n      - KAFKA_CREATE_TOPICS=test:1:1\\n      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\\n\\n  zookeeper:\\n    image: wurstmeister/zookeeper\\n    ports:\\n      - \\"2181:2181\\"\\n```\\n\\nFor this example, we’ll store the database contents locally in `./harperdb` directory. Also, note that we are not specifying `CLUSTERING_ENABLED=true` in docker-compose. This will break the initial startup, and we’ll configure via Harper Studio console.\\n\\nStart up the services via `docker-compose up -d`.\\n\\nNow we need to connect our local instance to Harper Studio. Specify the username and password from the docker compose file as well as port and host.\\n\\nAfter we log in, we can create a cluster user:\\n\\n```\\nCREATE USER cluster_user WITH PASSWORD 'password';\\nGRANT CLUSTER_ADMIN TO cluster_user;\\n```\\n\\nFinally, let’s create a schema and table. We’ll use our favorite `dev` schema and `dog` table respectively.\\n\\nCustom Functions Setup\\nHarperDB has an existing template utilizing the internal NATS stream and publishing to WebSockets: ​​https://github.com/HarperDB-Add-Ons/cf-template-websockets\\n\\nWe will modify this setup to publish to Kafka. But first, clone this repo into the `custom_function` directory of your HarperDB instance.\\n\\n```\\ncd harperdb/custom_functions\\ngit clone https://github.com/HarperDB-Add-Ons/cf-template-websockets.git\\n```\\n\\nTo get this working, rename `config.json.example` to `config.json` and update our NATS user and pass to one we created via HarperDB Studio. Finally, run `npm i` to install the dependencies.\\n\\nNOTE: HarperDB Studio cannot parse file names with multiple “.” so it may say “File does not exist”. Simply rename the files if you want to see the file contents on the console.\\n\\nNow restart HarperDB, and we can use the example client file (`client.example.js`) to test the WebSocket connection.\\n\\nOnce we start this function, we should see the message “open!” and adding new records to our `dog` table will print out the records.\\n\\nModifying to Publish to Kafka\\nInstead of publishing messages back to the WebSocket client, let’s now publish JSON messages to Kafka. To do so, install the Fastify Kafka library: `npm i fastify-kafkajs`.\\n\\nThen we can import and register the Kafka client.\\n\\n```\\nconst fastify = require('fastify')({ logger: true })\\nconst { Kafka } = require('kafkajs')\\n\\nfastify.register(require('fastify-kafkajs'), {\\n  clientId: 'my-app',\\n  brokers: ['localhost:9092']\\n})\\n```\\n\\nWe can now simply modify the `onPublishedMessage` function to publish to Kafka instead of writing back to the socket:\\n\\n```\\nconst kafka = fastify.kafka()\\nconst producer = kafka.producer()\\n\\nasync function onPublishedMessage(message) {\\n  const { operation, table, record } = message\\n  const key = `${table}_${record.id}`\\n  const value = JSON.stringify(record)\\n  await producer.send({\\n    topic: table,\\n    messages: [{ key, value }],\\n  })\\n}\\n```\\n\\nNow restart the server and connect to our WebSocket client again. Publish another message to HarperDB, and we can check that it has been published to Kafka by sshing into the Kafka container and using the `kafka-console-consumer` binary:\\n\\n```\\ndocker exec -it kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic dog --from-beginning\\n```\\n\\nWrapping Up\\nIn this tutorial, we saw how to use the internal NATS stream to listen to changes to data in HarperDB. We then created a Fastify route to subscribe to those tables and publish those new messages to WebSockets and Kafka. You can modify the `onPublishedMessage` method to publish to multiple topics and also run this WebSocket client in the background to emulate a Debezium-like experience.\\n\\nGet Started With HarperDB\\nHarperDB is a powerful database that can be used for a variety of use cases. Whether you need to implement CDC or build a real-time application, HarperDB has the features you need. To learn more, check out the HarperDB documentation and start building today."}
30	An artificial idiot, instead if ChatGPT doing exactly what it have been told, and took like an hour to implement :)	https://a13ks3y.github.io/ai/index.html	rejected	reddit	\N	\N	2023-06-07	\N
31	Windows Dev Drive - Storage Volume Customized for Developers with Improved Performance	https://www.infoq.com/news/2023/06/windows-dev-drive/	rejected	reddit	\N	\N	2023-06-07	\N
28	TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators	https://www.micahlerner.com/2023/04/16/telamalloc-efficient-on-chip-memory-allocation-for-production-machine-learning-accelerators.html	skipped	reddit	["Machine Learning","Memory Allo	micahlerner.comTelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning AcceleratorsPublished April 16, 2023\nFound something wrong?\nSubmit a pull request!\nDiscussion on Hacker NewsThis is one in a series of papers I’m reading from ASPLOS. These paper reviews can be delivered weekly to your inbox, or you can subscribe to the Atom feed. As always, feel free to reach out on Twitter with feedback or suggestions!TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning AcceleratorsA common pattern for integrating machine learning models with applications is deploying them to user devices, where the models run on local hardwareSee Apple’s guide to deploying transformers to the Apple Neural Engine. . Running models locally provides performance improvements (by eliminating communication with a cloud), while enabling private computing. Unfortunately, there are also challenges to running models locally because of diversity in hardware capabilities - a program that works well on the highest end modern phone may not perform optimally on previous generation devices.To effectively run on a user’s device, the software must efficiently use local resources, including memory. The problem of allocating memory has been studied extensivelySee Dynamic Storage Allocation: A Survey and Critical Review. , but ML models pose novel challenges. Specifically, memory allocation for ML models is a 2D bin-packing problemThere is a large amount of research on solving this problem - see Survey on two-dimensional packing and the Wikipedia reference. - unlike programs which grow and shrink their memory usage over time, ML models have strict requirements for memory allocations because certain parts of the model depend on others.Existing solutionsThe paper cites XLA, TFLite (optimized for mobile devices), and Apache TVM. for ML model memory allocation rely on heuristics or solvers (which can produce a closer to optimal output, but often take longer to run). The Telamalloc paper proposes a solution balancing a combination of heuristics and solvers. As a result, the research is able to tackle the challenge posed by wide variance in hardware capabilities, significantly reducing the time that it takes the model to allocate memory and run.What are the paper’s contributions?The paper makes three main contributions:How does the system work?The system takes the problem and turns it into a 2D-optimization problem, where memory blocks are assigned to different ranges of address space over time, based on the flow of the program.The authors aim the approach at tensor memory allocation both on mobile devices and in Tensor Processing UnitsSee An in-depth look at Google’s first Tensor Processing Unit (TPU). , a custom piece of hardware that is used for machine learning at scale.It is worth noting how well studied resource allocation is - the paper reviews the standard approach compilers follow to:1) take a graph representation of the model and perform various graph transformations, 2) divide the graph into smaller units of work (operators), and 3) map these operators to different units of hardware.The authors call the third component the mapping problem, and note it is fundamentally different than the problem they’re focused on, which they call the memory allocation problem:the mapping problem is concerned with determining which level of a memory hierarchy to map each buffer to, the memory allocation problem selects buffer locations within addressable scratchpad memories that are shared between multiple buffers with overlapping live ranges.Notably, the performance of solving the memory allocation problem impacts users. If the compilation of a model takes a long time, an application using a model won’t work. On the other hand, if the problem is solved quickly, but suboptimally, the model may not be able to successfully allocate memory (because it attempts to use too much memory).Problem FormulationThe authors represent the problem by providing a set of buffers with start, end, and size to the allocator, along with an upper limit to memory usage.The allocator then attempts to produce a solution mapping each buffer to an address, where none of the buffers overlap, and memory usage doesn’t exceed the specified limit.Memory Allocation HeuristicsThe paper describes three main heuristics for assigning buffers to addresses: best-fit, greedy, the approach Telamalloc implements (which is a combination of both).A best-fit allocator assigns buffers to address space in start time orderThe paper mentions that Tensorflow uses this strategy with its best-fit with coalescing (BFC) allocator. . The paper notes, “This approach works well if memory is abundant but fails if the memory budget is tight” because memory allocations of many blocks in a constrained space will be suboptimal.The greedy approach (used by TFLite) takes, “the end time into account to pick locations one buffer at time, while ensuring that it does not overlap with any previously allocated buffers.” Again, this approach doesn’t do well when memory is tight because it also produces suboptimal solutions.Lastly, there is the heuristic that Telamalloc implements, which takes into account the contention of a point of time (represented by the number of buffers that need to be assigned). Buffers with the highest contention are placed first at the lowest possible address (stored by keeping a “skyline” for each time period)This is reminiscent of the Skyline Problem! . If there are multiple buffers, the heuristic makes a decision based on other factors like the length of time a buffer exists.Solver-based ApproachesHeuristics for memory allocation have several downsides, including that their performance depends on the specific workload and problem difficulty - “once a heuristic has made a wrong decision that prevents it from solving the problem, it has no way to recover.” To address the shortcomings of heuristic failure, Telamalloc integrates a solver-basedIn particular, the paper relies on integer liner programming (ILP), described in more detail here. approach that represents the problem with several constraints, including all of the buffers taking up space at a given time can not exceed memory and buffers can not overlap.Telamalloc OverviewAs mentioned earlier, Telamalloc doesn’t solely rely on heuristics, nor solvers - heuristics get stuck on certain cases, and solvers can take too long. Normally solversThe paper specifically refers to a solver framework from Google, capable of representing a wide variety of constraints and problems. return the whole solution given an input and a set of constraints - instead, the program that guides the solver integrates interactively, reading the state of the solver for a particular buffer and making choices, then responding to feedback.At each step, the Search Heuristic chooses from the remaining unplaced blocksIt chooses blocks based on the following heuristics in order, “(1) The block with the longest lifetime (end-start time). (2) The block with the largest size. (3) The block with the largest area (i.e., size × lifetime).” , and “backtracks” to undo choices if a state it ends up in is invalid. It splits backtracking into “minor” and “major” based on how many steps need to be undone - the former corresponds to a single buffer placement, whereas the latter corresponds to undoing a whole line of choices (because the final state is invalid).The authors describe a number of optimizations to implement smart backtracking. Several of these focus on avoiding a return to the conditions that caused the initial backtrack. For example, on failure to satisfy constraints, the solver reports which placements occurred, so the search algorithm can unwind them quickly. Another example optimization is explicitly prioritizing buffers whose placement (or inability to place) led to a major backtrack - “this avoids cases where the solver got stuck by ignoring blocks that were important but not among the largest or longest-lived blocks”.Lastly, Telamalloc groups together buffers that contend with one another into phases, then runs the algorithm over each phase. This approach reduces the complexity of the problem, and allows choosing from a smaller set of candidate buffers when making choices.How is the research evaluated?The paper considers two main aspects of Telamalloc: microbenchmarks evaluating the algorithm in isolation, and measurements from compiling models / making memory allocations on a Pixel 6.The microbenchmarks consider the time to compute memory placements in the best and worst cases. In normal conditions, Telamalloc completes incredibly quickly (“≈10-100us for common problem sizes”). The worst case is represented by a large number of blocks (one thousand) with full overlap - in this situation, Telamalloc takes around 100000 ms, and each step takes significantly longer due to the strain placed on the solver (which needs to consider how a candidates interacts with many different potential placements).When comparing Telamalloc’s compilation of common models on the Pixel 6 running against a solver (which is capable of achieving near-optimal results given enough time), the memory allocations Telamalloc produces are nearly identical. Telamalloc is also able to achieve a, “median speedup of ≈ 4.7× across the benchmark”.ConclusionTelamalloc is an interesting paper because it discusses a combination of existing algorithms with optimizations tailored to improve user experiences relying on ML models. The paper also discusses using ML to make the performance of “smart” backtracking better - the idea of feeding in-the-wild data back into an algorithm to improve it over time is fascinating to me. This pattern also shows up in places like Java’s JIT compiler which takes data about a program’s performance and execution, then uses that to make the program better over time. Beyond the technical details of the paper, I also appreciated its focus on the impact to users - being able to compile models efficiently and successfully across a wide range of hardware is critical to making new AI-powered capabilities accessible to all.\nFollow me on\nTwitter or subscribe\nbelow to get future paper reviews. Published weekly.\nGet essays a bit faster\nI write about computer science research from the fields of\ndistributed systems and operating systems around once a week.\n	2023-06-07	title too long
13	ORM - data model vs domain model	https://gertgoeman.com/posts/orm-data-model-vs-domain-model/	skipped	reddit	["ORM"]	Gert Goeman\nORM - data model vs domain model 2023-06-05 There’s been quite a lot of discussion lately about whether or not to use an ORM. As someone who’s been both proponent and opponent of the use of ORM’s over the years, I figured I’d write a blog post about my current opinion on the matter (which may change again a couple of times in the future ;-)).\nWhy use an ORM\nWriting data-access code can be tedious and there’s a lot of boilerplate code involved.\nMaking a mistake can easily cause serious and hard to debug issues (like connection pool exhausting, …). ORM’s have already implemented and tested (always pick a commonly used ORM) all this functionality for you.\nThere’s often a lot of mapping code required to map between the object model and the sql statements. It’s easy to make mistakes here. Typo’s, or simply forgetting to include the right columns in your queries, can cause your queries to return incorrect data. To make things worse, everytime you change your database schema, you have to make sure you manually update your object-model and queries, which once again can lead to bugs.\nMost ORM’s solve these issues by providing you with automatic mapping and type-safe queries. There’s still room for error (obviously the ORM needs to be configured correctly), but at least the compiler will give you an error if you’re querying a non-existing column.\nData model vs domain model\nORM’s try to solve the object relational impedance mismatch [1]. The idea is that there’s often no one-on-one relation between your database tables and your domain model. Depending on your application architecture, there can be a variety of reasons for this: normalization, inheritance, …\nWhen I’m talking about a domain model, I don’t necessarily mean it in the typical DDD sense either. It can be any object model that you use in your application code.\nThe main issue with this type of complicated mapping is that it’s not easy for the ORM to generate optimized queries. If you’re querying for an object that maps to multiple database tables, the resulting sql statement will contain a bunch of joins. If your object is part of an inheritance chain, it’ll probably add some additional predicates to those join statements. The point is, you have little control over what the eventual query will looks like because it’s based on the configured mapping.\nI’m a big proponent of avoiding these issues by separating your domain model (or whatever you use) from your data model. By defining a data model that’s a one-on-one reflection of your database tables, you can maintain control of your sql statements without giving up the advantages of using an ORM.\nBecause every class corresponds to a single table you easily predict what your sql statements will looks like and which joins will be produced. There won’t be any hidden magic involved since the ORM doesn’t have to do any complex mapping.\nIf you want to use a DDD approach and define aggregates, you can create separate objects to make up your domain model. You can map the classes from the data model to the domain model whenever necessary. When using a strongly typed language, these kind of mappings are easy to test and refactor becaus they’re checked by the compiler. A similar approach can be taken if you use another architecture (such as CQRS).\n[1] https://en.wikipedia.org/wiki/Object%E2%80%93relational_impedance_mismatch #programming #orm #database ORM - data model vs domain model 2023-06-05\nThere’s been quite a lot of discussion lately about whether or not to use an ORM. As someone who’s been both proponent and opponent of the use of ORM’s over the years, I figured I’d write a blog post about my current opinion on the matter (which may change again a couple of times in the future ;-)).Why use an ORMWriting data-access code can be tedious and there’s a lot of boilerplate code involved.Making a mistake can easily cause serious and hard to debug issues (like connection pool exhausting, …). ORM’s have already implemented and tested (always pick a commonly used ORM) all this functionality for you.There’s often a lot of mapping code required to map between the object model and the sql statements. It’s easy to make mistakes here. Typo’s, or simply forgetting to include the right columns in your queries, can cause your queries to return incorrect data. To make things worse, everytime you change your database schema, you have to make sure you manually update your object-model and queries, which once again can lead to bugs.Most ORM’s solve these issues by providing you with automatic mapping and type-safe queries. There’s still room for error (obviously the ORM needs to be configured correctly), but at least the compiler will give you an error if you’re querying a non-existing column.Data model vs domain modelORM’s try to solve the object relational impedance mismatch [1]. The idea is that there’s often no one-on-one relation between your database tables and your domain model. Depending on your application architecture, there can be a variety of reasons for this: normalization, inheritance, …When I’m talking about a domain model, I don’t necessarily mean it in the typical DDD sense either. It can be any object model that you use in your application code.The main issue with this type of complicated mapping is that it’s not easy for the ORM to generate optimized queries. If you’re querying for an object that maps to multiple database tables, the resulting sql statement will contain a bunch of joins. If your object is part of an inheritance chain, it’ll probably add some additional predicates to those join statements. The point is, you have little control over what the eventual query will looks like because it’s based on the configured mapping.I’m a big proponent of avoiding these issues by separating your domain model (or whatever you use) from your data model. By defining a data model that’s a one-on-one reflection of your database tables, you can maintain control of your sql statements without giving up the advantages of using an ORM.Because every class corresponds to a single table you easily predict what your sql statements will looks like and which joins will be produced. There won’t be any hidden magic involved since the ORM doesn’t have to do any complex mapping.If you want to use a DDD approach and define aggregates, you can create separate objects to make up your domain model. You can map the classes from the data model to the domain model whenever necessary. When using a strongly typed language, these kind of mappings are easy to test and refactor becaus they’re checked by the compiler. A similar approach can be taken if you use another architecture (such as CQRS).[1] https://en.wikipedia.org/wiki/Object%E2%80%93relational_impedance_mismatch #programming #orm #database 	2023-06-06	Object-Relational Mapping (ORM) has been a topic of discussion in the programming community for quite some time now. While some developers advocate for its use, others are against it. In this article, we will explore the advantages and disadvantages of using an ORM and provide insights into the data model vs domain model debate.\n\n## Why Use an ORM?\n\nWriting data access code can be tedious and time-consuming. There is a lot of boilerplate code involved, and making a mistake can easily cause serious and hard-to-debug issues like connection pool exhaustion. ORM's have already implemented and tested all this functionality for you, making it easier to write data access code.\n\nORM's provide automatic mapping and type-safe queries, which reduces the risk of errors. There is often a lot of mapping code required to map between the object model and the SQL statements. It's easy to make mistakes here, such as typos or forgetting to include the right columns in your queries. Every time you change your database schema, you have to make sure you manually update your object model and queries, which can lead to bugs. ORM's solve these issues by providing automatic mapping and type-safe queries.\n\n## Data Model vs Domain Model\n\nORM's try to solve the object-relational impedance mismatch, which means there's often no one-to-one relation between your database tables and your domain model. Depending on your application architecture, there can be a variety of reasons for this, such as normalization and inheritance.\n\nWhen we talk about a domain model, we don't necessarily mean it in the typical Domain-Driven Design (DDD) sense. It can be any object model that you use in your application code. The main issue with this type of complicated mapping is that it's not easy for the ORM to generate optimized queries. If you're querying for an object that maps to multiple database tables, the resulting SQL statement will contain a bunch of joins. If your object is part of an inheritance chain, it'll probably add some additional predicates to those join statements. You have little control over what the eventual query will look like because it's based on the configured mapping.\n\nWe advocate for separating your domain model from your data model. By defining a data model that's a one-on-one reflection of your database tables, you can maintain control of your SQL statements without giving up the advantages of using an ORM. Every class corresponds to a single table, so you can easily predict what your SQL statements will look like and which joins will be produced. There won't be any hidden magic involved since the ORM doesn't have to do any complex mapping.\n\nIf you want to use a DDD approach and define aggregates, you can create separate objects to make up your domain model. You can map the classes from the data model to the domain model whenever necessary. When using a strongly typed language, these kinds of mappings are easy to test and refactor because they're checked by the compiler. A similar approach can be taken if you use another architecture, such as Command Query Responsibility Segregation (CQRS).\n\nIn conclusion, ORM's can be advantageous in reducing the time and effort required to write data access code. However, it's essential to consider the data model vs domain model debate and separate them to maintain control of your SQL statements and optimize your queries.
24	Jailer: Database Subsetting and Relational Data Browsing Tool 15.0 released.	https://github.com/Wisser/Jailer	skipped	reddit	["Database","Jailer"]	 Wisser / Jailer Public\nNotifications\nFork 68 Star 1.6k Database Subsetting and Relational Data Browsing Tool. wisser.github.io/jailer License Apache-2.0 license 1.6k stars 68 forks Star\nNotifications Code Issues 0 Pull requests 0 Discussions Actions Projects 0 Wiki Security Insights More Code Issues Pull requests Discussions Actions Projects Wiki Security Insights Wisser/Jailer This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. master Switch branches/tags Branches Tags Could not load branches Nothing to show {{ refName }} default View all branches Could not load tags Nothing to show {{ refName }} default View all tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? Cancel Create 2 branches 278 tags Code Local Codespaces Clone HTTPS GitHub CLI Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Open with GitHub Desktop Download ZIP Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio Code Your codespace will open once ready. There was a problem preparing your codespace, please try again. Latest commit Wisser 15.0.1.2 … 832103a Jun 5, 2023 15.0.1.2 832103a Git stats 4,792 commits Files Permalink Failed to load latest commit information. Type Name Latest commit message Commit time .github removed March 8, 2023 10:50 admin added jackson-core-2.15.1.jar June 1, 2023 10:00 bookmark Demo Bookmark July 21, 2021 12:26 config 12.3 April 18, 2022 10:15 datamodel reverted May 27, 2023 14:24 docs added FAQ June 5, 2023 12:11 example added some examples December 19, 2018 07:51 extractionmodel moved December 19, 2018 06:28 layout new layout February 9, 2021 15:27 lib added jackson-core-2.15.1.jar June 1, 2023 10:00 maven-artifacts new October 3, 2020 10:27 src 15.0.1.2 June 5, 2023 12:51 template retrieve table/column comments, WIP November 8, 2022 13:06 tmp ensure existence January 10, 2019 08:32 .classpath switching to slf4j April 17, 2022 09:00 .gitignore 12.5.1.3 July 14, 2022 13:54 .project separated ui from subsetter February 9, 2021 15:29 Jailer.exe classpath update April 18, 2022 10:23 README.md added news February 2, 2023 08:45 build.xml new main class May 17, 2022 11:34 demo-sakila.mv.db reverted May 27, 2023 14:24 demo-scott-subset.mv.db H2 v2.1.212 May 14, 2022 15:16 demo-scott.mv.db reverted May 27, 2023 14:24 driverlist.csv updated db2 jdbc download URL April 21, 2023 13:59 jailer.bat switching to slf4j April 17, 2022 09:00 jailer.jar 15.0.1.1 June 5, 2023 12:51 jailer.sh fixed path separator April 19, 2022 11:23 jailerGUI.bat new main class May 17, 2022 11:34 jailerGUI.sh new main class May 17, 2022 11:34 license.txt Apache License 2.0 February 9, 2021 15:20 releasenotes.txt release notes update June 5, 2023 09:49 View code Jailer Database Tool Features Supported Databases News Installation Building Contact Contributors Code Contributors Financial Contributors Individuals Organizations README.md Jailer Database Tool\nJailer is a tool for database subsetting and relational data browsing.\nThe Subsetter creates small slices from your database (consistent and referentially intact)\nas SQL (topologically sorted), DbUnit records or XML.\nIdeal for creating small samples of test data or for local problem analysis with relevant production data.\nThe Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables. DataBrowser.mp4\nMore Videos\nFeatures\nExports consistent and referentially intact row-sets from your productive database\nand imports the data into your development and test environment.\nImproves database performance by removing and archiving obsolete data without violating integrity.\nGenerates topologically sorted SQL-DML, hierarchically structured XML and DbUnit datasets.\nData Browsing. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.\nSQL Console with code completion, syntax highlighting and database metadata visualization.\nA demo database is included with which you can get a first impression without any configuration effort.\nSupported Databases\nThanks to the JDBC technology used, any DMBS is in principle supported. For best results, specific additional support features are useful, however. These are available for:\nPostgreSQL\nOracle\nMySQL\nMariaDB\nMicrosoft SQL Server\nIBM Db2\nSQLite\nSybase\nAmazon Redshift\nFirebird\nInformix Dynamic Server\nH2\nExasol\nNews\n2023-02-03 Thanks to deep analysis of statements, the SQL console can now relate the result of queries to the source tables and display them accordingly. In addition, this technique also allows filter conditions to be dynamically added to arbitrary SQL queries.\n2022-01-01 Comprehensive redesign and modernization of the entire user interface. New Look & Feel FlatLaf.\n2021-02-04 Cycles in parent-child relationships will be detected and broken. Thus, such data can be exported by deferring the insertion of nullable foreign keys.\n2020-02-04 The Jailer engine is published in Maven repository. https://mvnrepository.com/artifact/io.github.wisser/jailer-engine\n2019-02-01 The new "Model Migration Tool" allows you to easily find and edit the newly added associations if the data model has been extended after the last change to this extraction model.\n2018-04-26 The new feature "Analyze SQL" analyzes SQL statements and proposes association definitions. This allows to reverse-engineer the data model based on existing SQL queries.\n2018-03-06 SQL Console with code completion, syntax highlighting and database metadata visualization.\n2017-05-10 New API provides programmatic access to the data export and import functionality. https://wisser.github.io/Jailer/api.html\n2017-03-30 Improved filter management. Templates allows you to define rules for assigning filters to columns. Filters on primary key columns will automatically be propagated to the corresponding foreign key columns. https://wisser.github.io/Jailer/filters.html\n2015-12-04 Data can now also be exported directly to a schema of the same database. This ensures optimal performance.\n2015-10-23 Rows can alternatively be collected in a separate embedded database. This allows exporting data from read-only databases.\n2014-07-20 Implemented the "Subset by Example" feature: Use the Data Browser to collect all the rows to be extracted and let Jailer create a model for that subset. https://wisser.github.io/Jailer/subset-by-example.html\n2014-04-15 A Data Browser has been introduced. Navigate bidirectionally through the database by following foreign-key-based or user-defined relationships.\nInstallation\nUse the installation file "Jailer-database-tools-n.n.n.msi" (for Windows) or "jailer-database-tools_n.n.n-x64.deb" (for Linux).\nUnless you want to use your own Java installation. Then unzip the file "jailer_n.n.n.zip". See also https://wisser.github.io/Jailer/faq.html#multiuser\nTo start the tool from the unpacked zip:\nOn windows platform execute "Jailer.exe". You can also start "jailerGUI.bat".\nOn Unix/Linux platform execute the script "jailerGUI.sh" or use "java -jar jailer.jar"\nBuilding\nClone the git repository:\ngit clone https://github.com/Wisser/Jailer.git\nTo build the tool you can just use ant: ( https://ant.apache.org )\ncd Jailer\nant\nContact\nHome: https://github.com/Wisser/Jailer or http://jailer.sourceforge.net/\nForum: https://sourceforge.net/p/jailer/discussion/\nSupport: rwisser@users.sourceforge.net\nContributors\nCode Contributors\nThis project exists thanks to all the people who contribute.\nFinancial Contributors\nBecome a financial contributor and help us sustain our community. [Contribute]\nIndividuals\nOrganizations\nSupport this project with your organization. Your logo will show up here with a link to your website. [Contribute] About Database Subsetting and Relational Data Browsing Tool. wisser.github.io/Jailer Topics mysql java testing export gui sql database frontend jdbc extract postgresql oracle mssql redshift db2 sqlserver subsetting subsetter jailer Resources Readme License Apache-2.0 license\nStars 1.6k stars\nWatchers 32 watching\nForks 68 forks Report repository Releases 200 Jailer 15.0.1 Latest Jun 5, 2023 + 199 releases\nSponsor this project opencollective.com/Jailer Contributors 4 Wisser Wisser swerner0 vsgfe hedgehog-jacek Jacek Languages Java 99.6% Shell 0.1% HTML 0.1% Batchfile 0.1% NSIS 0.1% SQLPL 0.0% Database Subsetting and Relational Data Browsing Tool. LicenseWisser/JailerName already in use Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio CodeYour codespace will open once ready.There was a problem preparing your codespace, please try again.Latest commitGit statsFiles README.md Jailer Database ToolJailer is a tool for database subsetting and relational data browsing.The Subsetter creates small slices from your database (consistent and referentially intact)\nas SQL (topologically sorted), DbUnit records or XML.Ideal for creating small samples of test data or for local problem analysis with relevant production data.The Data Browser lets you navigate through your database following the relationships (foreign key-based or user-defined) between tables.More VideosFeaturesSupported DatabasesThanks to the JDBC technology used, any DMBS is in principle supported. For best results, specific additional support features are useful, however. These are available for:NewsInstallationUse the installation file "Jailer-database-tools-n.n.n.msi" (for Windows) or "jailer-database-tools_n.n.n-x64.deb" (for Linux).Unless you want to use your own Java installation. Then unzip the file "jailer_n.n.n.zip". See also https://wisser.github.io/Jailer/faq.html#multiuserTo start the tool from the unpacked zip:BuildingClone the git repository:To build the tool you can just use ant: ( https://ant.apache.org )ContactContributorsCode ContributorsThis project exists thanks to all the people who contribute.\nFinancial ContributorsBecome a financial contributor and help us sustain our community. [Contribute]IndividualsOrganizationsSupport this project with your organization. Your logo will show up here with a link to your website. [Contribute]\nAbout Database Subsetting and Relational Data Browsing Tool. TopicsResourcesLicenseStarsWatchersForks Releases 200\nSponsor this project Contributors 4\nLanguagesFooterFooter navigation	2023-06-07	title too long
25	SVG versus Canvas: Which technology to choose and why?	https://www.jointjs.com/blog/svg-versus-canvas	skipped	reddit	["SVG","Canvas"]	SVG versus Canvas: Choosing the right technology for your web applicationSetting the sceneChoosing what technology to use can often be a time consuming process. A dilemma can often arise if you are unsure of the benefits or drawbacks of each prospective choice. This is understandable as the last thing you want is to choose a particular technology, and later realize you should have gone with a different option for one reason or another.Debates about SVG versus Canvas are no different. A quick search on the topic will return a multitude of articles, and stackoverflow questions comparing the two technologies. It can be a daunting task to piece together all of the information in the hope of making a decision you can be confident in.In the following article, we will cover some of the same topics you have probably read about elsewhere, but hope to provide you with a more nuanced view. We will also shed some light on what implications those topics have for developers and business owners in practical terms.Before getting into the weeds of the debate, in an article titled SVG versus Canvas, it would be remiss not to at least mention some of the basic differences between the two technologies at the outset, so let’s get to it!Vectors and PixelsTwo households, both alike in creating visual content for the web, but fundamentally different in their approach. SVG (Scalable Vector Graphics) is an XML based markup language used to describe 2D vector graphics. Canvas, on the other hand, allows users to draw on the HTML Canvas element via a JavaScript API.Canvas code example:-- CODE language-svg --<canvas id="canvas"></canvas><script>const canvas = document.getElementById("canvas");const ctx = canvas.getContext("2d");ctx.fillStyle = "green";ctx.fillRect(10, 10, 150, 100);</script>‍SVG code example:-- CODE language-svg --<svg> <rect width="150" height="100" fill="green" /></svg>From the beginning, SVG was developed as an open standard by W3C (the World Wide Web Consortium), that means it was designed specifically to work well with other web standards. Generally, you can think of SVG as declarative drawing instructions that can be added alongside your HTML.The history of Canvas is a little more muddled. Originally introduced by Apple, it was later standardized by WHATWG (the Web Hypertext Application Technology Working Group). As Canvas was originally created as a proprietary element by Apple, its adherence to web standards is questionable, and still lacking in important areas to this day.The most pertinent difference is how each technology presents its content. Canvas is raster based, meaning it’s arrays of pixels arranged on a grid, while SVG is vector based, meaning it uses mathematical metadata when describing a graphic. The advantage of the latter is that when zooming or scaling, SVG will maintain its integrity, remaining crisp and clean on different-sized monitors and resolutions. Canvas content, including text, doesn’t preserve clarity when resized.A second, and at first glance, seemingly less important difference is that each SVG element is present in the DOM (Document Object Model), while Canvas is represented as a sole element. The reason why this is relevant will become more apparent later.Now that we have established that both technologies can draw stuff, let’s dig a little deeper on some important aspects to consider when making decisions about SVG and Canvas.Accessibility: What happens in Canvas, stays in CanvasA blazingly fast ™ website with incredible content is all for nothing if it’s not accessible to its users. The same idea can be applied to creating information-rich visual graphics, and the technology you choose to create those graphics can play an integral role in determining just how accessible they are.If you want to talk about people with disabilities, they represent 16% of the global population according to the World Health Organization. While all of those people obviously don’t use assistive software to navigate the web, the number is difficult to ignore if you are a business owner who wants to broaden their user base, or if you are worried about the huge influx of lawsuits regarding the inaccessible web in recent years. Not only is accessibility a good business practice, but it’s the right thing to do in order to create the best experience for everybody.In light of this, how do SVG and Canvas stack up against each other in terms of accessibility? Earlier, we mentioned that Canvas is a single element within the DOM. To put it frankly, this has huge repercussions for accessibility. Canvas doesn’t provide any information about the drawn content, nor does it expose any information to accessibility tools. MDN puts it rather bluntly:“In general, you should avoid using canvas in an accessible website or app.”In contrast, as SVG and all its content is in the DOM, it naturally conveys semantic meaning, and assistive technology can access sub-elements, text and links. As SVG was designed with web standards in mind, it can be further enhanced by accessibility standards such as ARIA (Accessible Rich Internet Applications). That means what is visually represented in a graphic, can also be conveyed in the markup itself, allowing assistive software to get access to this information.Is it possible to convey HTML Canvas content as markup? This is actually an important question in terms of Canvas and accessibility. The short answer is … it depends. Does your Canvas display some simple static shapes, or does it contain a complex interactive diagram? W3C (The World Wide Web Consortium), the primary web standards organization, says the following:“When authors use the canvas element, they must also provide content that, when presented to the user, conveys essentially the same function or purpose as the canvas's bitmap. This content may be placed as content of the canvas element. The contents of the canvas element, if any, are the element's fallback content.”Fallback content in the form of a text description can certainly convey the characteristics of a simple static shape. When it comes to a dynamic diagram, providing fallback content that conveys the same function becomes a little more troublesome. In fact, the HTML specification says best practice is to include focusable elements as fallback content corresponding to each focusable part of the Canvas. At this point, you might be asking yourself, why not just use SVG in the first place?Canvas is often sold as “not having to deal with the overhead of the DOM”. In terms of accessibility, that “overhead” is your users.Performance, a bird’s eye viewIn a large proportion of articles and stackoverflow answers, performance is the main reason provided for choosing Canvas over SVG. The topic is often reduced to a one-liner such as “If you want to render 10,000 elements, use Canvas!”. What more do you need to know, right?An incorrect assertion that you may also come across in these discussions is that Canvas always outperforms SVG, but that’s not necessarily the case. SVG is actually more performant with a small number of objects, or over a large surface area. If you work with a small surface area, or a large number of objects, Canvas is out in front.Other points that could be relevant are the type of content and the manner in which users will interact with your application, or if anything can be done to alleviate the performance advantage that Canvas has in some use cases.Consider a scatter plot with 10,000 data points. Generally, this type of chart provides an overview of some dataset by plotting 2 variables, and suggests some correlation between them. The important information is gleaned from looking at the points as a whole, not at each one individually.In terms of UI (User Interface), you can assume the user doesn’t need to zoom in on each point, or that each circle would have child elements such as text or images. If you wanted to introduce a timeline to animate each point over time, you can certainly start reaching for Canvas as your tool of choice.The type of information you need to visualize more often than not determines the manner in which it’s displayed. What if each data point needs to be an interactive card-like element with text, images, and action buttons? How will your users interact with this content?Apart from it being less likely to have 10,000 data points you need to visualize in this manner, how important would it be to view all these elements as a whole? In this instance, the detail of each individual element is probably what users are most interested in, and the amount of these elements that could be viewed simultaneously in a useful way is certainly limited.This begs the question, is there any strategy that could be employed to improve performance? On the remote chance that someone needed to view 10,000 information-rich elements as a whole, can a simple view of each element be rendered instead? After all, the element details wouldn’t be legible anyway. What about rendering only a subset of elements at a given time? In the age of “lazy loading”, a good use case would be to only render visible elements within the viewport, saving valuable milliseconds.In fact some libraries, including JointJS, provide similar solutions already. JointJS can avoid rendering content which is not visible to the user. When deciding which technology to use, it may be worth your time asking similar questions. If the primary advantage of Canvas is somewhat diminished, are you willing to sacrifice all of the other benefits SVG provides?More meaningful testing with SVGEnsuring your application behaves as expected is an integral part of the development process. In order to ascertain that your graphic functions correctly, debugging problems and testing functionality will be ingrained in your workflow. Once again, when comparing Canvas and SVG, it’s difficult not to return to the fact that Canvas is a solitary HTML element in the DOM. This has numerous practical implications which may affect the speed or reliability of delivering your software.Naturally, one of the first resources a developer will use when presented with a visual bug is to inspect an element in the DOM with developer tools. It’s a familiar process that can reveal a lot of crucial information quickly, such as if the element is even present in the DOM in the first place, or if it has the expected characteristics, and so on. This all contributes to identifying problems swiftly, and addressing them as needed.As a lone Canvas element acts on the principle of what you see is what you get, developers are deprived of this point of contact with their applications, and will often have to pore over longer Canvas code to find an issue, losing valuable time in doing so.In order to minimize the amount of debugging you have to do, you’ll probably want to introduce testing in your workflows. Since SVG and Canvas are both used to create graphics, it’s usually imperative that you test the UI (User Interface) of your applications. Some of the popular E2E (end-to-end) testing frameworks that are used to automate this process are Cypress, Playwright, and Selenium.One thing these frameworks have in common is that they work under the assumption that DOM elements are accessible. Elements are located via the concept of “Selectors”, a catch-all term that can include CSS selectors, accessibility roles, or even text. The fact that none of this information is readily available when using Canvas should immediately raise alarm bells.If attempting to do E2E testing with Canvas, you’ll first likely have to write lots of custom code to locate the Canvas “elements”, then have to capture events on the Canvas element itself, or use some workaround to pretend to interact with elements without actually generating any DOM events.The goal of E2E testing is to test applications from a real user’s perspective. Any makeshift solution that could be implemented with the aim of emulating this process for Canvas will simply never be a convincing substitute. The bottom line is, if you want to do authentic end-to-end testing of your UI, SVG is the way to go.What the heck is Foreign Object?It might seem strange to focus on a specific SVG element as an advantageous reason for choosing SVG, but it’s not without merit. If a requirement for your graphic is to have interactive elements, developers are generally a habitual bunch. They will often reach for technologies they are familiar with, and in web land, the dominant force in this regard is still good old HTML.‍Foreign Object essentially allows the addition of HTML in your SVG elements. Embedding HTML text, creating basic interactive elements like buttons, or working with HTML inputs is all made a straightforward process. If you want to work with HTML on Canvas, a more unconventional approach such as rendering HTML on top of an underlying element will have to be adopted.While this technique can be successful, it comes with a lot of additional complexity, such as the need to keep dimensions and position of the HTML in sync with the element itself. Taking advantage of foreign object in SVG allows you to create HTML-rich elements while avoiding some of the difficulties of other approaches.Don’t believe us? Learn more about foreign object in SVG in our JointJS tutorial, and see what you can create.CSS and SVG play nicelyCSS is, and will remain one of the main pillars of the modern web. The ability to introduce uniform changes via CSS results in more maintainable applications. As SVG and CSS play nicely together, all of that CSS goodness extends to your SVG, allowing for even more modular code.This integration is so seamless, it’s often overlooked just how easy it is to update some SVG styles using CSS pseudo-classes, such as :hover. A painful amount of code has to be written in order to achieve similar functionality with Canvas.Need to get creative with some more vibrant interactions? Animation is now ubiquitous online, and CSS allows the implementation of complex animations with relative ease, often outperforming any JavaScript equivalents. These animation techniques can be utilized with all DOM elements, including SVG.CSS is not a static technology either. The evolution of CSS also means that your SVG code can evolve alongside it, benefiting from new standards and functionality, often adopted from CSS preprocessors. Component based architecture is ever present in the web world. With great new features like native CSS nesting, this architecture will be a mainstay for years to come, allowing for even more organization within SVG applications. As you can see, when evaluating a debate about Canvas versus SVG, those two technologies by themselves don’t paint the whole picture.Bits and BobsNot all facets of a debate about SVG and Canvas need to remain within the realm of the development world, there are also some external aspects which may be worth considering when choosing a technology, such as team structure, or how your content is viewed by search engines.Teams usually consist of many roles, each member has their own strengths and type of work they focus on. Developers are often not the strongest designers, and as a result many teams will include both. The path to creating beautiful looking software, usually begins at the feet of designers. If you are lucky enough to have designers at your disposal, that means a lot of work can be entrusted to them, freeing up developers to focus on what they do best.Need a beautiful set of icons for your startup idea? If working with SVG, this entire process can be taken care of by your design team without a lot of input from developers. Modern design tools like Figma or Adobe illustrator allow the export of SVG, so an icon set can be passed to your development team upon completion, and embedded in your application with relative ease. As SVG is just markup, developers can update any other SVG properties as needed without any back and forth. As images for Canvas will always be static, extra coordination among team members is needed to ensure all image properties are in order before handing them over. Not to mention that you’ll need a little extra JavaScript to add the images to Canvas.SEO (Search Engine Optimization) isn’t the first thing that pops into one’s mind when comparing Canvas and SVG, but it can certainly be beneficial to your business. SVG and Canvas both excel at complex graphics, but only one of them will be searchable and indexable. At a basic level, when users are presented with a large amount of text, it’s possible even for most novices to utilize a command/ctrl + F command on their keyboard in order to find some relevant text. This simply isn’t possible with Canvas. A nice side effect of the DOM is that the text in SVG will also be indexable. Presenting crucial information in graphic form could be incredibly desirable depending on the use case. Using search engines like Google, your users will be able to discover text content through your graphics when using SVG.Last thoughts on SVGSVG and Canvas are both established technologies, so whichever one you choose, you will be supported by huge ecosystems and communities. Based on the points raised in this article, it won’t surprise you to find that our preferred tool of choice is SVG, but Canvas certainly has its use cases too. In our opinion, the practical and ethical benefits of SVG far outweigh what Canvas has to offer, and if the performance benefit of Canvas can be mitigated with techniques such as virtual rendering, the question remains if you are willing to leave all of the advantages that SVG has to offer on the table? Lastly, if some milliseconds will make or break your application, it may be worth your time taking a look at the WebGL/WebGPU APIs instead of the Canvas API.AuthorsRead nextDemo Wednesday: Searchable SitemapDemo Wednesday: JointJS+ ChatGPT MindMapDemo Wednesday: Decision TreeSpeed up your development with a powerful library	2023-06-07	title too long
5	Github: ShaderSearch - Shadertoy Search Tool (And IMHO a Good Example of HTML/Javascript/CSS Programming)	https://github.com/mrmcsoftware/ShaderSearch	skipped	reddit	["HTML","JavaScript","CSS"]	 mrmcsoftware / ShaderSearch Public\nNotifications\nFork 0 Star 0 An HTML file to search for and play Shadertoy shaders by using shadertoy.com's API. www.youtube.com/mrmcsoftware/videos 0 stars 0 forks Star\nNotifications Code Issues 0 Pull requests 0 Actions Projects 0 Security Insights More Code Issues Pull requests Actions Projects Security Insights mrmcsoftware/ShaderSearch This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. main Switch branches/tags Branches Tags Could not load branches Nothing to show {{ refName }} default View all branches Could not load tags Nothing to show {{ refName }} default View all tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? Cancel Create 1 branch 0 tags Code Local Codespaces Clone HTTPS GitHub CLI Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Open with GitHub Desktop Download ZIP Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio Code Your codespace will open once ready. There was a problem preparing your codespace, please try again. Latest commit mrmcsoftware Add info about Github Page (live demo) … dff7b43 Jan 31, 2023 Add info about Github Page (live demo) dff7b43 Git stats 11 commits Files Permalink Failed to load latest commit information. Type Name Latest commit message Commit time .editorconfig add .editorconfig January 30, 2023 04:35 README.md Add info about Github Page (live demo) January 31, 2023 02:50 index.html Try to make Github Pages to work January 31, 2023 02:24 jquery.min.js Try to make Github Pages to work January 31, 2023 02:24 screenshot.png first commit January 30, 2023 04:18 screenshot2.png first commit January 30, 2023 04:18 screenshot3.png first commit January 30, 2023 04:18 stoysearch.html Change key usage January 31, 2023 01:58 View code Shadertoy Search Tool Setup and Running User Interface Mouse Button Actions URL Parameters Text Boxes, Some Buttons, and Some Checkboxes Shadertoy.com Links Limitations and Why You Would Want to Use This Tool Possible Improvements Author README.md Shadertoy Search Tool\nThis is an HTML file for searching shadertoy for shaders and playing them.\nIt will only find shaders with the "Public + API" privacy setting (Refer\nto the Limitations section).\n(I purposely used an older browser for the screenshots because I prefer the 3d\nlook over the flat button look of some modern browsers.)\nSetup and Running\nOPTIONAL: If you want a gradual transition between Dark and Light modes,\nuncomment the line containing transition: color 300ms, background-color 300ms;\nThere are various ways of starting up this webpage. If your browser is in your\nsearch path you could simply do the following, for example, at a command prompt:\nfirefox stoysearch.html\nOr you could start your browser and use your browser's "Open File" (or\nequivalent) menu option. Or you could use the file:/// URI/protocol and\nspecify the whole path.\nFor a live demo, go to https://mrmcsoftware.github.io/ShaderSearch but do\ntry to use your own copy instead.\nNote: Ignore index.html and jquery.min.js . They are only there\nto make Github Pages work.\nUser Interface\nWhile using this tool, you can press the ? (Help) button to get the\nfollowing information.\nMouse Button Actions\nClick left mouse button on a shader thumbnail to run the shader in place.\nTip: when you do this, move the pointer out of the thumbnail area to allow the\ncontrol panel to go away. If you desire the control panel to always be shown,\nhold down the Ctrl key on keyboard while doing the above mouse click. If\nyou want any possible sound muted, hold down both Ctrl and Shift while\ndoing the above mouse click. If you want to see a larger version of the\nthumbnail (and not run the shader), hold down the Shift key while doing the\nabove mouse click. Note: once you run a shader, these options are no longer\navailable for that shader unless you reload the page via either the\n"Reload page" option of the "Results Per Page" button or any other\nfeature that refreshes the page. Larger versions are also shown if\n"1 per page" is selected with the Results Per Page button. Note:\nthumbnails may all be different sizes, due to the fact that the thumbnail is\ncreated at the point of their shader's run (and characteristics (i.e. window\nsize) of that run) in which their shader is saved. By the way, be aware that\nsome shaders don't have thumbnails, so sometimes you might see the standard\nbrowser's no image icon.\nClick the middle mouse button (unless you specified "nomiddle=true" in\nthis page's URL, in which case it's the right mouse button) on a shader\nthumbnail to show a message box containing information about that shader. If\nyou want to see the shader tabs (code) associated with that shader, hold down\nthe Ctrl key while doing the above mouse click. Note: you may have to tell\nyour browser to allow popups for this page.\nURL Parameters\nUse these like this, for example (If specifying this on a terminal commandline,\nyou probably will need to escape the special characters, depending on your OS\n(for example: stoysearch.html?dark=false\\&sthumb=true if using Linux,\n"stoysearch.html?dark=false&sthumb=true" if using Windows)):\nstoysearch.html?dark=false&sthumb=true&sort=3&search=sdf\ndark=false - Turn off dark mode\nslinks=false - Turn off showing shadertoy.com links\nsthumb=true - Use smaller thumbnails\nnomiddle=true - Use right mouse button instead of middle\nhelp=false - Don't display the help button\nrangeselect=off - Don't show range page buttons\nshowfilters=true - Show the search filter checkboxes\nsort={number} - Select the search sort ({number} is dropdown item number (1-5))\nnpp={number} - Select number of shaders per page ({number} is dropdown item number (1-8))\ncols={number} - Select number of columns (search results)\nfilters={string} - Specify list of search filters to apply ({string} is a comma separated list with same names as the checkboxes)\nsearch={string} - Search using the specified {string} text\nText Boxes, Some Buttons, and Some Checkboxes\nTo search for shaders using a text string, type it into the top-left text box.\nYou can either press "Enter" on keyboard after you type the string or click\nthe Search button. If you leave the box blank, all shaders with the API\nenabled will be returned.\nOnce you have some shader results, shader result number (of the first shader on\nthe page) and page number text boxes will be shown. If you would like to jump\nto a specific shader result number or page number, type it into the appropriate\nbox and press Enter. Note: you can also click the range page buttons (if\nrangeselect=off isn't specified) to jump to a new page. And, of course,\nthe Next Page and Previous Page buttons can be clicked to navigate the\npages.\nNarrow down the search results by selecting various filters. If the filter\ncheckboxes aren't already visible, click "Show Filters". Select any\ncombination (can select more than 1) of filters. Then run the search.\nNote: soundinput is the same as shadertoy.com's Microphone, soundoutput is GPU\nSound, and musicstream is Soundcloud.\nShadertoy.com Links\nIf "Show links" is checked, each shader search result will have an "L"\nnext to it. Click on the L to open up a new browser tab to it's\nshadertoy.com page.\nLimitations and Why You Would Want to Use This Tool\nThis search tool will ONLY find/show shaders that have the API permission set.\nThis isn't due to this search tool, it is due to shadertoy.com's API and the\nobvious desire to allow the shader's author to decide how/if their shader can\nbe accessed. Because of this, this search tool isn't a replacement for\nshadertoy.com by any means - just a useful tool. For example, because it\ndoesn't automatically run any shader, it can be faster, and perhaps more\nimportantly safer (ever have your computer or graphics card crash due to a\nshader? I have), to go through the search results.\nPossible Improvements\nThere are some things that could be improved (either by me or by someone else\ninclined to do so). One improvement would be to remove the reliance on\njQuery. The hardest part would be to replace getJSON with the\nequivalent AJAX (XMLHttpRequest) code or even the fetch function. The rest\nwould be easy.\nYou might notice that my javascript code isn't always consistent in how it does\ncertain things. This is partly because I believe in reusing code I've already\nwritten and partly because I wanted to show that there are many different ways\nof doing the same thing. Much of github is meant for teaching/learning after\nall. My only requirement is that the different ways must not require the most\nup-to-date browsers. But you might want consistency, so feel free to make it\nconsistent.\nThis tool currently gets jquery.min.js from Google (ajax.googleapis.com).\nYou may want to change that to wherever you choose to get jQuery from (or you\nmay want to change which version of jQuery is used). You can even download\njquery.min.js to the same directory as stoysearch.html and avoid getting it\nfrom the internet every time.\nAuthor\nMark Craig\nhttps://www.youtube.com/MrMcSoftware About An HTML file to search for and play Shadertoy shaders by using shadertoy.com's API. www.youtube.com/MrMcSoftware/videos Topics javascript css search html api jquery shadertoy search-interface shadertoy-api Resources Readme\nStars 0 stars\nWatchers 1 watching\nForks 0 forks Report repository Releases No releases published Packages 0 No packages published Languages HTML 100.0% An HTML file to search for and play Shadertoy shaders by using shadertoy.com's API. mrmcsoftware/ShaderSearchName already in use Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio CodeYour codespace will open once ready.There was a problem preparing your codespace, please try again.Latest commitGit statsFiles README.md Shadertoy Search ToolThis is an HTML file for searching shadertoy for shaders and playing them.\nIt will only find shaders with the "Public + API" privacy setting (Refer\nto the Limitations section).\n(I purposely used an older browser for the screenshots because I prefer the 3d\nlook over the flat button look of some modern browsers.)Setup and RunningOPTIONAL: If you want a gradual transition between Dark and Light modes,\nuncomment the line containing transition: color 300ms, background-color 300ms;There are various ways of starting up this webpage. If your browser is in your\nsearch path you could simply do the following, for example, at a command prompt:Or you could start your browser and use your browser's "Open File" (or\nequivalent) menu option. Or you could use the file:/// URI/protocol and\nspecify the whole path.For a live demo, go to https://mrmcsoftware.github.io/ShaderSearch but do\ntry to use your own copy instead.Note: Ignore index.html and jquery.min.js . They are only there\nto make Github Pages work.User InterfaceWhile using this tool, you can press the ? (Help) button to get the\nfollowing information.Mouse Button ActionsClick left mouse button on a shader thumbnail to run the shader in place.\nTip: when you do this, move the pointer out of the thumbnail area to allow the\ncontrol panel to go away. If you desire the control panel to always be shown,\nhold down the Ctrl key on keyboard while doing the above mouse click. If\nyou want any possible sound muted, hold down both Ctrl and Shift while\ndoing the above mouse click. If you want to see a larger version of the\nthumbnail (and not run the shader), hold down the Shift key while doing the\nabove mouse click. Note: once you run a shader, these options are no longer\navailable for that shader unless you reload the page via either the\n"Reload page" option of the "Results Per Page" button or any other\nfeature that refreshes the page. Larger versions are also shown if\n"1 per page" is selected with the Results Per Page button. Note:\nthumbnails may all be different sizes, due to the fact that the thumbnail is\ncreated at the point of their shader's run (and characteristics (i.e. window\nsize) of that run) in which their shader is saved. By the way, be aware that\nsome shaders don't have thumbnails, so sometimes you might see the standard\nbrowser's no image icon.Click the middle mouse button (unless you specified "nomiddle=true" in\nthis page's URL, in which case it's the right mouse button) on a shader\nthumbnail to show a message box containing information about that shader. If\nyou want to see the shader tabs (code) associated with that shader, hold down\nthe Ctrl key while doing the above mouse click. Note: you may have to tell\nyour browser to allow popups for this page.URL ParametersUse these like this, for example (If specifying this on a terminal commandline,\nyou probably will need to escape the special characters, depending on your OS\n(for example: stoysearch.html?dark=false\\&sthumb=true if using Linux,\n"stoysearch.html?dark=false&sthumb=true" if using Windows)):Text Boxes, Some Buttons, and Some CheckboxesTo search for shaders using a text string, type it into the top-left text box.\nYou can either press "Enter" on keyboard after you type the string or click\nthe Search button. If you leave the box blank, all shaders with the API\nenabled will be returned.Once you have some shader results, shader result number (of the first shader on\nthe page) and page number text boxes will be shown. If you would like to jump\nto a specific shader result number or page number, type it into the appropriate\nbox and press Enter. Note: you can also click the range page buttons (if\nrangeselect=off isn't specified) to jump to a new page. And, of course,\nthe Next Page and Previous Page buttons can be clicked to navigate the\npages.Narrow down the search results by selecting various filters. If the filter\ncheckboxes aren't already visible, click "Show Filters". Select any\ncombination (can select more than 1) of filters. Then run the search.Note: soundinput is the same as shadertoy.com's Microphone, soundoutput is GPU\nSound, and musicstream is Soundcloud.Shadertoy.com LinksIf "Show links" is checked, each shader search result will have an "L"\nnext to it. Click on the L to open up a new browser tab to it's\nshadertoy.com page.Limitations and Why You Would Want to Use This ToolThis search tool will ONLY find/show shaders that have the API permission set.\nThis isn't due to this search tool, it is due to shadertoy.com's API and the\nobvious desire to allow the shader's author to decide how/if their shader can\nbe accessed. Because of this, this search tool isn't a replacement for\nshadertoy.com by any means - just a useful tool. For example, because it\ndoesn't automatically run any shader, it can be faster, and perhaps more\nimportantly safer (ever have your computer or graphics card crash due to a\nshader? I have), to go through the search results.Possible ImprovementsThere are some things that could be improved (either by me or by someone else\ninclined to do so). One improvement would be to remove the reliance on\njQuery. The hardest part would be to replace getJSON with the\nequivalent AJAX (XMLHttpRequest) code or even the fetch function. The rest\nwould be easy.You might notice that my javascript code isn't always consistent in how it does\ncertain things. This is partly because I believe in reusing code I've already\nwritten and partly because I wanted to show that there are many different ways\nof doing the same thing. Much of github is meant for teaching/learning after\nall. My only requirement is that the different ways must not require the most\nup-to-date browsers. But you might want consistency, so feel free to make it\nconsistent.This tool currently gets jquery.min.js from Google (ajax.googleapis.com).\nYou may want to change that to wherever you choose to get jQuery from (or you\nmay want to change which version of jQuery is used). You can even download\njquery.min.js to the same directory as stoysearch.html and avoid getting it\nfrom the internet every time.AuthorMark Craig\nhttps://www.youtube.com/MrMcSoftwareAbout An HTML file to search for and play Shadertoy shaders by using shadertoy.com's API. TopicsResourcesStarsWatchersForks Releases Packages 0\nLanguagesFooterFooter navigation	2023-06-06	title too long
6	Building startup with AI cofounder (day 21): building an MVP using GraphQL	https://knowlo.co/blog/day-21-building-an-mvp-signup-flow-and-graphql/	skipped	reddit	["GraphQL"]	Day 21: Building an MVP – signup flow and GraphQLCofounderGPT and I got stuck on Day 19 with AppSync JavaScript resolvers. Everything worked great until we tried to deploy the app and realized that JavaScript resolvers do not work for unit resolvers! Currently, they only support pipeline resolvers.Resolving the resolvers issueAs mentioned in the previous article, besides JavaScript resolvers, we have the following options: VTL resolvers and Lambda functions.VTL resolvers are hard to write and even harder to test. I have some experience with them, and CofounderGPT can probably help. But using VTL is the opposite of fun, so I don’t want to use them unless I have to.On the other side, writing a Lambda function is not hard. However, using a Lambda function to read or write some simple values in the DynamoDB table is overkill. Lambda functions require a bit more code, have a scaling limit (that can be increased, and are also slightly slower than VTL and JavaScript resolvers (because of their initialization or cold start).So, do we have any other options?We do! And it’s a bit crazy, but it should work perfectly fine for our scenario. What if we wrap unit resolvers in a pipeline resolver?Let’s pause here and try to explain unit vs. pipeline resolvers first.Each GraphQL query and mutation require a resolver. Resolver is a function that is a glue between GraphQL API and the rest of the world (data sources). GraphQL does not care about resolver complexity. These functions can do whatever they want if they return the data in an expected format. AWS AppSync has two global types of resolvers: you can write a Lambda function and do whatever you want, or you can write a resolver with VTL and now JavaScript, where you have certain limitations, but you get lower latency, less code, and faster scaling in return.With VTL and JavaScript resolvers, one resolver can only talk to one data source and run a single supported operation. For example, you can read from one DynamoDB table or send one HTTP request. These resolvers are called unit resolvers.If you need to read the data from multiple data sources, you can chain VTL or JavaScript resolvers in a pipeline resolver. Each pipeline resolver has a before template (prepares the request data for units), an after template (packs the response in the response), and a series of functions (each is similar to a unit resolver, with some minor API differences).Because JavaScript resolvers support only pipeline resolvers, we can make a pipeline resolver wrapper for a single function (or unit resolver). The downside of this approach is slightly increased complexity and a slightly slower GraphQL operation (I can guess this because I never measured it, but it’s probably up to 50ms slower). But the upside is that we can still use TypeScript instead of VTL for resolvers. And trust me, that’s a huge benefit!This approach sounds like a hack, and it is a hack, but it seems that we are not the only ones with the same idea! Multiple articles on the internet write about the same approach. For example, the “Hitting the road with AppSync JavaScript resolvers” article uses a CDK construct to “hide” this complexity. This approach makes even more sense when we know that AWS AppSync plans to support unit resolvers in the future. In this case, once the AWS AppSync team releases JavaScrip unit resolver support, we can simply change one part of our code, and all of our resolvers will become unit resolvers!We’ll do the same: I created a new folder in the “lib” folder of the backend project and named it “stack” (I know, I am “very” creative). And then, I created a new file inside with the name “javascript-resolver.ts” (the full path is “backend/lib/stack/javascript-resolver.ts”) and the following content:CofounderGPT is much better at explaining code snippets than I am, so he’s its explanation:And that’s it. We can now use our new construct instead of graphQLApi.createResolver in the main Knowlo CDK stack file. Here’s the new code for our resolver:If we run the npm run cdk deploy -- --parameters LogLevel=DEBUG command to redeploy our stack, everything works fine now. The problem is successfully resolved.Can CofounderGPT generate resolvers?Can I ask CofounderGPT to generate resolvers? AWS AppSync released JavaScript resolvers after the ChatGPT training cutoff point, but we might be able to “teach” CofounderGPT the JavaScript resolver syntax. Let’s try! I used the browsing plugin for the following prompt.Slobodan: Here’s the documentation for AWS AppSync JavaScript resolvers for DynamoDB data source: https://docs.aws.amazon.com/appsync/latest/devguide/js-resolver-reference-dynamodb.html. Can you help me writing JavaScript resolvers in TypeScript for my GraphQL query if I provide the GraphQL schema and the DynamoDB database design?CofounderGPT: Sure, I’d be happy to help you with that. Please provide the details of your GraphQL schema and the DynamoDB database design so I can understand your requirements better and assist you in writing the TypeScript resolvers for AWS AppSync.This was a new conversation, so I sent a GraphQL schema and a DynamoDB table design to CofounderGPT and reminded it that we store all the data in a single table.Slobodan: Can you generate an AppSync JavaScript resolver for the `createProject` mutation? Remember, I need a TypeScript version. And you can access the AppSync JavaScript resolvers documentation here: https://docs.aws.amazon.com/appsync/latest/devguide/js-resolver-reference-dynamodb.html.CofounderGPT: Sure, I can help you create a TypeScript version of the AppSync JavaScript resolver for the createProject mutation. Here’s how you might do it based on the provided GraphQL schema and DynamoDB single-table design:This resolver function begins by destructuring the input argument from the context. It then generates an id for the new project using the context.util.autoId() function and the current timestamp with context.util.time.nowISO().Next, it creates a project object that includes all the required attributes for a project, including the id, the knowledge base provider, the knowledge base S3 path, the description, the creation timestamp, and an archived status that is initially set to false.Finally, the function returns a PutItem operation request for DynamoDB, with the key being a combination of the generated id for the project and the attribute values being all the properties of the project object.The context.util.dynamodb.toMapValues function is used to convert the JavaScript objects into a format that DynamoDB can store.Please note that this is a very basic resolver function. Depending on your specific needs, you may need to add more complex logic, such as error handling, additional attribute validation, or further manipulation of the data before it is stored in DynamoDB​1​.Slobodan: Excellent! I also need a response function in the same file. See the documentation here: https://docs.aws.amazon.com/appsync/latest/devguide/resolver-reference-overview-js.html.CofounderGPT: Based on the provided AWS AppSync documentation, the response handler for your createProject mutation would look like this in TypeScript:This function takes the context object ctx as a parameter and simply returns the result of the mutation operation. The ctx.result object contains the data returned from the DynamoDB operation, which will be the newly created project in this case.This function will be called by AppSync after the request handler function has completed, and its returned value will be used as the response data for the createProject mutation. Since the mutation is expected to return a Project type, the data returned by this function should align with the fields defined in the Project type in your GraphQL schema.It’s worth noting that this function is quite simple and just returns the result of the mutation operation as is. In a more complex application, you might want to perform additional processing in the response handler function, such as transforming the data, performing additional error handling, or implementing custom logic based on the result of the mutation operation​1​.Slobodan: Ok, not bad. But here’s an example for the response function. Can we do something similar to this?CofounderGPT: The example you provided is a more complex response function that performs additional processing on the result of the DynamoDB operation before returning it. Here’s the equivalent function in TypeScript:In this function:This is a common pattern for response functions in AppSync resolvers, where you might want to transform the data returned from DynamoDB and handle any errors that occurred during the operation.Slobodan: You can find more about PutItem API here: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html Can you return only the result without version?CofounderGPT: Based on the AWS documentation, the PutItem operation in DynamoDB creates a new item or replaces an old item with a new item (including all the attributes). However, the PutItem operation itself does not return any data in the response. It does not include the item’s details that have been written to the table​1​.As for AWS AppSync, when a PutItem operation is performed through an AppSync resolver, the data that is written to DynamoDB is automatically converted to GraphQL and JSON primitive types, and is made available in the context result (context.result)​3​.In your provided example response function, it seems like you are trying to remove the version from the result before returning it. This would work if the PutItem operation actually returned the item data. However, as per the DynamoDB documentation, this is not the case. Therefore, you wouldn’t have a version attribute or any other attribute of the item in ctx.result after a PutItem operation.To achieve what you want (i.e., to get the item without the version after a PutItem operation), you would need to perform a separate GetItem operation after PutItem, and then you can modify the returned item as needed before returning it in your response function.Please let me know if you have any more questions or if there’s something else you’d like me to do!Everything CofounderGPT generated and said makes sense. But I need to test it to be sure. However, to test GraphQL queries, I need to go a step back and finish the signup and login process in the frontend app.Signup and loginI did a few things in a frontend application to enable the signup and login flow. A step-by-step explanation would take a lot of space here, so I’ll walk you through the most important parts. Feel free to reach out to me on twitter or Linkedin if you have any questions.At the end, the frontend folder structure looked like this:One of the essential things that I changed is the router in the App.tsx file. It has the following code now:In this code:The AppShell component looks like this:CofounderGPT’s explanation:A few notes:The signup process has two steps: a signup form and a confirmation step (a code is sent via email). This flow is defined in the Signup component. Here’s the code for that component:In this code snippet:This code provides a secure signup process for a user, including validation to ensure that the information entered is correct and a second step to verify the user. It also handles errors and some edge cases, like needing to resend the validation code. There’s still some work here to make it production-ready, but it’s good enough to test the GraphQL part.The signup page looks similar to the following screenshot:Once a user enters the data, the app will send a verification code via email and present the following screen:Once the account is verified, a user is automatically logged in. The dashboard still looks the same (the content is hidden because it’s not connected to GraphQL yet):Finally, the log in screen looks like the following screenshot:These pages give us enough to test and connect GraphQL requests, which we’ll do in the next step of the MVP-building process.ScoreboardSince I don’t trust Google to charge my card automatically for ads because I’ve seen them abuse this power many times in Vacation Tracker. I made a pre-payment of $150 CAD and asked it to stick within that prepaid budget.Time spent today: 8hTotal time spent: 145hInvestment today: $0 USDTotal investment: $1,141.54 USDBeta list subscribers: 48Paying customers: 0Revenue: $0What’s nextThe next step is connecting the GraphQL requests and showing the actual data in our app. CofounderGPT will help me to write AppSync resolvers faster and to write React components using TypeScript and Tailwind CSS.Like this:RelatedShare:Slobodan StojanovicSlobodan Stojanović is the CTO and co-founder of Vacation Tracker, an application that simplifies employee leave and absence management for organizations. He is based in Belgrade and is the JS Belgrade meetup co-organizer.\nSlobodan is an AWS Serverless Hero, Claudia.js core team member, and co-author of the “Serverless Applications with Node.js” book, published by Manning Publications.Comments are closedUnlock Exclusive Knowlo Launch AccessJoin our community today and get early access to the AI-powered knowledge base revolution, co-created by CofounderGPT!Latest PostsArchivesCategoriesQuick LInkCommunityCopyright 2023 Knowlo. All Rights Reserved by Vacation Tracker.	2023-06-06	title too long
8	Dataset of most (all widely used + some others which are no longer used) programming languages and related data. (including installation commands, desc, usage, speed, etc)	https://github.com/merwin-asm/LanguageIndex	skipped	reddit	["Programming Languages"]	 merwin-asm / LanguageIndex Public\nNotifications\nFork 1 Star 5 Dataset of most (about all) programming languages with information related to it.. in JSON raw.githubusercontent.com/merwin-asm/languageindex/main/main.json License MIT license 5 stars 1 fork Star\nNotifications Code Issues 0 Pull requests 0 Actions Projects 0 Security Insights More Code Issues Pull requests Actions Projects Security Insights merwin-asm/LanguageIndex This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. main Switch branches/tags Branches Tags Could not load branches Nothing to show {{ refName }} default View all branches Could not load tags Nothing to show {{ refName }} default View all tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? Cancel Create 1 branch 0 tags Code Local Codespaces Clone HTTPS GitHub CLI Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Open with GitHub Desktop Download ZIP Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio Code Your codespace will open once ready. There was a problem preparing your codespace, please try again. Latest commit merwin-asm Formatted the json … 60863be Jun 6, 2023 Formatted the json 60863be Git stats 13 commits Files Permalink Failed to load latest commit information. Type Name Latest commit message Commit time LICENSE Initial commit June 6, 2023 19:34 README.md Update README.md June 6, 2023 20:38 main.json Formatted the json June 6, 2023 22:03 View code LanguageIndex warning Contents of the file Downloading README.md LanguageIndex\nDataset of most (all widely used + some others which are no longer used) programming languages.\nIt provides a basic description, if the language is OOP or not ,if it is statically or dynamically typed , information about the speed of the language, what it is mostly used for, the marketshare of the programming language and installation command for Linux (the command maynot work on all distributions), MacOS and Windows(some of the languages couldn't be installed using commandline on Windows hence are left null).\nwarning\nThis Index may containing wrong information , wrong/harmful installation commands and so on. This list have to be used with care.\nThe list may not be complete and information which we werent able to get was set as null. The data was generated also with the help of tgpt.\nIt is under MIT license\nContents of the file\nStored as Json\nTotal number of languages - > 656\nIf any info wasnt found it is set to null\nStructure :\n{\n"lang_1" : {\n"description" : "................",\n"OOP" : "true / false (if its object oriented programming language)",\n"type" : "statically typed / dynamically typed",\n"speed" : "very fast/fast/medium/slow",\n"mostly_used_for" : "..............",\n"market_share" : ".................",\n"installation" : {\n"windows" : "....",\n"linux" : ".....",\n"macOS": ".....", }, },\n}\nDownloading\nSimple download from here.\ngit clone https://github.com/merwin-asm/LanguageIndex.git raw data : https://raw.githubusercontent.com/merwin-asm/LanguageIndex/main/main.json About Dataset of most (about all) programming languages with information related to it.. in JSON raw.githubusercontent.com/merwin-asm/LanguageIndex/main/main.json Topics programming-language list information installation details programming-languag-list Resources Readme License MIT license\nStars 5 stars\nWatchers 1 watching\nForks 1 fork Report repository Releases No releases published Packages 0 No packages published Dataset of most (about all) programming languages with information related to it.. in JSON Licensemerwin-asm/LanguageIndexName already in use Use Git or checkout with SVN using the web URL. Work fast with our official CLI. Learn more about the CLI. Sign In Required Please sign in to use Codespaces. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching GitHub Desktop If nothing happens, download GitHub Desktop and try again. Launching Xcode If nothing happens, download Xcode and try again. Launching Visual Studio CodeYour codespace will open once ready.There was a problem preparing your codespace, please try again.Latest commitGit statsFiles README.md LanguageIndexDataset of most (all widely used + some others which are no longer used) programming languages.\nIt provides a basic description, if the language is OOP or not ,if it is statically or dynamically typed , information about the speed of the language, what it is mostly used for, the marketshare of the programming language and installation command for Linux (the command maynot work on all distributions), MacOS and Windows(some of the languages couldn't be installed using commandline on Windows hence are left null).warningThis Index may containing wrong information , wrong/harmful installation commands and so on. This list have to be used with care.\nThe list may not be complete and information which we werent able to get was set as null. The data was generated also with the help of tgpt.\nIt is under MIT licenseContents of the fileDownloadingSimple download from here.raw data : https://raw.githubusercontent.com/merwin-asm/LanguageIndex/main/main.jsonAbout Dataset of most (about all) programming languages with information related to it.. in JSON TopicsResourcesLicenseStarsWatchersForks Releases Packages 0\nFooterFooter navigation	2023-06-06	## Introduction\n\nAs a developer, staying up-to-date with the latest programming languages and frameworks is crucial. However, it can be challenging to keep track of all the new and old programming languages, their features, and installation commands. Fortunately, Merwin-asm has created a comprehensive dataset of programming languages that contains information related to them. The dataset is available in JSON format and includes details such as basic description, whether the language is OOP or not, if it is statically or dynamically typed, information about the speed of the language, what it is mostly used for, the market share of the programming language, and installation commands for Linux, MacOS, and Windows.\n\n## Dataset Details\n\nThe dataset contains information about most programming languages, including some that are no longer used. It includes installation commands for Linux, MacOS, and Windows, but some languages may not be installed using command line on Windows. The dataset also warns that the information provided may not be complete and may contain wrong/harmful installation commands. Therefore, it should be used with care. The data was generated with the help of tgpt and is under the MIT license.\n\nThe dataset is stored as a JSON file and has a total of 656 programming languages. If any information was not found, it is set to null. The structure of the dataset is as follows:\n\n```json\n{\n  "lang_1": {\n    "description": "................",\n    "OOP": "true / false (if it's an object-oriented programming language)",\n    "type": "statically typed / dynamically typed",\n    "speed": "very fast/fast/medium/slow",\n    "mostly_used_for": "..............",\n    "market_share": ".................",\n    "installation": {\n      "windows": "....",\n      "linux": ".....",\n      "macOS": "....."\n    }\n  }\n}\n```\n\n## How to Access the Dataset\n\nThe dataset is available on GitHub and can be downloaded using the following command:\n\n```bash\n$ git clone https://github.com/merwin-asm/LanguageIndex.git\n```\n\nAlternatively, you can access the raw dataset using the following link:\n\n```bash\nhttps://raw.githubusercontent.com/merwin-asm/LanguageIndex/main/main.json\n```\n\n## Conclusion\n\nMerwin-asm's comprehensive dataset of programming languages is a valuable resource for developers who want to keep up with the latest programming languages and frameworks. It provides essential information about programming languages, including installation commands, speed, and market share. However, it is essential to use the dataset with care, as it may contain wrong/harmful installation commands and incomplete information.\n
27	Refactoring Code — Taming the Spaghetti	https://admirlive.medium.com/refactoring-code-taming-the-spaghetti-325b68452dd0?source=friends_link&sk=4cc79a6b0aa4df218c9c6008647ccf0b	published	reddit	["Refactoring","Code"]	Refactoring Code — Taming the SpaghettiAdmir MujkicFollow--ListenShareMany of us have encountered situations where we find ourselves confronted with a complex and tangled piece of code, resembling a bowl of spaghetti. However, there is no need to fret! This blog post endeavors to explore the realm of refactoring, whereby we untangle our chaotic code and transfigure it into an elegant, modular, and comprehensible solution.Before we start, what is spaghetti code?Spaghetti code refers to source code that is messy and difficult to understand. It has a tangled structure, making it hard to maintain and likely to have errors.Let’s explore an example of spaghetti code:Let’s approach the situation step by step and try to untangle the mess.Identify the ProblemOne of the initial issues we face is the lack of meaningful variable names. Meaningful and descriptive names greatly enhance code readability. To untangle the code, our first step is to rename variables so that they accurately represent their purpose, making the code self-explanatory. This best practice improves our own understanding and helps future developers working on the code.The code’s lack of modularity is evident in the oversized Main function that performs multiple tasks. To tackle this problem, we will break down the Main function and identify separate functionalities. These will be extracted into their own functions or classes. This approach allows us to group related logic together, promote code reusability, and enhance the overall structure of the codebase.Another important aspect that needs improvement is the code’s limited error handling. Effective error handling is crucial for identifying and gracefully recovering from unexpected situations. To address this, we will examine potential exceptions that may occur during the code’s execution. We will then implement appropriate error handling mechanisms, such as try-catch blocks, to ensure the code handles exceptions smoothly and provides helpful error messages.The code contains numerous complex if-else conditions, making it difficult to read and understand. To simplify this, we will utilize techniques like switch statements, polymorphism, or design patterns such as the strategy pattern. These approaches help streamline conditional logic and reduce its complexity. By doing so, we improve the code’s readability, maintainability, and make it easier to modify in the future.Break it DownTo begin untangling the code, our first step is to split it into smaller, manageable chunks. This allows us to focus on specific functionalities and improve modularity. By breaking down the code into smaller parts, we can isolate and understand individual components more effectively, making it easier to maintain and enhance the codebase.To improve the modularity of the code, we can start by separating the logic responsible for determining the number type and the logic for printing numbers into separate functions or classes.Use Descriptive NamesTo enhance code readability and maintainability, it’s essential to give meaningful names to variables and methods. Let’s apply this practice to the code by assigning appropriate names to the relevant elements.From:To:Use Design Patterns and PrinciplesTo enhance code readability and reduce the complexity of conditionals, we can employ the Strategy Pattern. The Strategy Pattern allows us to encapsulate different algorithms or strategies and dynamically select one at runtime. Here’s how we can apply the Strategy Pattern to replace complex conditionals:From:To:By utilizing the Strategy Pattern, we achieve a more modular and maintainable code structure. It simplifies the complex conditional logic, improves code readability, and makes it easier to extend or modify the behavior in the future.Improve Error HandlingThe original code had minimal error handling. Let’s improve that.From:To:Final TouchesWe can enhance readability by using string interpolation.From:To:And there you have it! Our final, refactored code is clear, modular, and “easy” to understand.Cyclomatic ComplexityCyclomatic Complexity (CC) is a metric that measures the complexity of a program. It quantifies the number of independent paths through the program’s source code. For simple if-else constructs, you can estimate the cyclomatic complexity by counting the decision points (such as if, while, for statements) and adding one.In the old code:Adding them up and adding one, the cyclomatic complexity is 9.In the new code:The cyclomatic complexity of the new code is 4.Reducing cyclomatic complexity simplifies the code, reduces the likelihood of errors, and improves maintainability.FinallyIn conclusion, through refactoring, we’ve made our code more maintainable, easier to understand, and reduced the cyclomatic complexity from 9 to 4. It’s a solid win for any developer.Remember, great code is not about how complex you can make it, but how simple you can make it. As Albert Einstein said, “Everything should be made as simple as possible, but no simpler”. Happy coding!P.S. If you believe I can help with something, don’t hesitate to contact me, and I will reach you as soon as possible. admir.m@penzle.comCheers! 👋References:Clean Code in C#: Refactor your legacy C# code base and improve application performance by applying…Clean Code in C#: Refactor your legacy C# code base and improve application performance by applying best practices…www.amazon.comRefactoring: Improving the Design of Existing Code (2nd Edition) (Addison-Wesley Signature Series…Refactoring: Improving the Design of Existing Code (2nd Edition) (Addison-Wesley Signature Series (Fowler)) [Fowler…www.amazon.com----Written by Admir MujkicAdmir combined engineering expertise with business acumen to make a positive impact & share knowledge. Dedicated to educating the next generation of leaders.HelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams	2023-06-07	{"title":"Skip.","article":""}
11	Where are Objects Allocated in C#? Understanding Heap and Stack	https://gevorgchobanyan.medium.com/where-are-objects-allocated-in-c-understanding-heap-and-stack-951febcac8fe?source=friends_link&sk=c7b5377291d59ffb2846c0bfebf997ba	published	reddit	["C#"]	Sign upSign InSign upSign InWhere are Objects Allocated in C#? Understanding Heap and StackGevorg ChobanyanFollowITNEXT--ListenShareIn the world of C# programming, understanding where objects are allocated is crucial for optimizing memory usage and ensuring efficient code execution. In this comprehensive guide, we will delve into the two primary memory locations for object allocation in C#: the heap and the stack. By grasping these concepts, you’ll be equipped with valuable insights that can help you write high-performance C# applications.Heap vs. Stack: Unveiling the Allocation ParadigmIn C#, objects can be stored either on the heap or the stack, and comprehending the differences between these two memory locations is essential.Allocating Objects: Reference Types and Value TypesNow that we understand the basics of heap and stack memory, let’s explore how different types of objects are allocated in C#.Starting with C# 7.2, you can declare a struct as a ref struct, ensuring it is always allocated on the stack and preventing it from being declared inside reference types.You can check more about ref structure here.Reference Semantics for Value TypesWhile value types are typically passed by copy, C# provides mechanisms to allow accessing value types by reference, thus introducing reference semantics. Let’s explore some of the keywords and constructs that enable this:Memory Management: Garbage Collection and the HeapOne of the critical aspects of managing memory in C# is understanding how the heap memory is freed up. Unlike objects on the stack that are automatically deallocated when the corresponding stack frame is popped, objects on the heap require the intervention of the garbage collector.When an object on the heap no longer has any references pointing to it, it becomes eligible for garbage collection. At a certain point, the garbage collector kicks in, interrupts running threads, invokes the finalizers of the objects it needs to collect (on a special finalizer thread), and then marks the memory as available for reuse.The Large Object Heap (LOH): A Special Memory SpaceIn C#, the heap is further divided into the Small Object Heap (SOH) and the Large Object Heap (LOH). This separation is primarily to address memory fragmentation and optimize memory usage.Understanding the Small Object Heap (SOH) and the Large Object Heap (LOH) is vital for memory optimization in your C# applications, as it allows you to make informed decisions regarding object size and memory allocation.----Written by Gevorg ChobanyanITNEXT🚀 Unity Game Developer | Crafting immersive experiences, pushing boundaries, and turning dreams into interactive realities. Let's level up together! 🎮✨HelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams	2023-06-06	{"title":"Understanding Object Allocation in C#: Heap and Stack","article":"\\nAs a C# developer, understanding where objects are allocated is crucial for optimizing memory usage and ensuring efficient code execution. In this article, we will delve into the two primary memory locations for object allocation in C#: the heap and the stack. By grasping these concepts, you’ll be equipped with valuable insights that can help you write high-performance C# applications.\\n\\nHeap vs. Stack: Unveiling the Allocation Paradigm\\n\\nIn C#, objects can be stored either on the heap or the stack, and comprehending the differences between these two memory locations is essential. The stack is used for storing value types and reference types that are declared as local variables or method parameters. When a method is called, a new stack frame is created to store the method's local variables and parameters. When the method returns, the stack frame is popped, and the memory is automatically deallocated.\\n\\nOn the other hand, the heap is used for storing reference types that are created using the \\"new\\" keyword, and it's the responsibility of the garbage collector to free up the memory when it's no longer needed. The heap is a larger memory space than the stack, and it's not organized in a specific order like the stack.\\n\\nAllocating Objects: Reference Types and Value Types\\n\\nNow that we understand the basics of heap and stack memory, let’s explore how different types of objects are allocated in C#. Value types are allocated on the stack, and they are passed by value. Reference types, on the other hand, are allocated on the heap, and they are passed by reference.\\n\\nStarting with C# 7.2, you can declare a struct as a ref struct, ensuring it is always allocated on the stack and preventing it from being declared inside reference types. This can be useful for performance-critical scenarios where you want to avoid heap allocations.\\n\\nReference Semantics for Value Types\\n\\nWhile value types are typically passed by copy, C# provides mechanisms to allow accessing value types by reference, thus introducing reference semantics. The \\"ref\\" and \\"out\\" keywords can be used to pass value types by reference. The \\"in\\" keyword can be used to pass value types by reference without allowing them to be modified.\\n\\nMemory Management: Garbage Collection and the Heap\\n\\nOne of the critical aspects of managing memory in C# is understanding how the heap memory is freed up. When an object on the heap no longer has any references pointing to it, it becomes eligible for garbage collection. At a certain point, the garbage collector kicks in, interrupts running threads, invokes the finalizers of the objects it needs to collect (on a special finalizer thread), and then marks the memory as available for reuse.\\n\\nThe Large Object Heap (LOH): A Special Memory Space\\n\\nIn C#, the heap is further divided into the Small Object Heap (SOH) and the Large Object Heap (LOH). This separation is primarily to address memory fragmentation and optimize memory usage. The SOH is used for objects smaller than 85,000 bytes, while the LOH is used for larger objects.\\n\\nUnderstanding the Small Object Heap (SOH) and the Large Object Heap (LOH) is vital for memory optimization in your C# applications, as it allows you to make informed decisions regarding object size and memory allocation.\\n\\nIn conclusion, understanding object allocation in C# is crucial for writing high-performance applications. By grasping the differences between the heap and the stack, you can optimize memory usage and ensure efficient code execution. Knowing how value types and reference types are allocated, and how memory is managed through garbage collection, can help you write more efficient and effective code."}
14	Easy K8S Connectivity for Local Utils	https://metalbear.co/blog/easy-k8s-connectivity-for-local-utils/	published	reddit	["Kubernetes"]	Easy K8S Connectivity for Local UtilsPosted June 4, 2023 by Tal Zwick - 6 Min ReadWith the new targetless mode of mirrord, you can run a program locally on your machine, and mirrord will forward\nnetwork connections initiated by the program to the cluster, such that the program gets the connectivity it would\nhave if it were deployed to the cluster. Together with the secret sauce of in-cluster DNS resolution, you can run a\nprogram on your computer and have it access cluster-internal services that do not have any external IP. The\nprogram would also have access to third-party services that are open to IPs from your cluster, but not to your\nlocal IP.This can be useful for different cases, notably for running utility tools with access to the cluster, or for debugging\nnew services.Running utility programs with access to your cluster #When you run an application with mirrord, and the application connects to a network address or domain name, DNS\nresolution as well as the network connection will be done from the cluster. This lets you run tools locally for\nconfiguring or testing your services on the cluster.Let’s see what this looks like with a little practical example. We have a Kafka cluster set up on Kubernetes1:With mirrord we can use utility tools to manage and test the services on the cluster, with the same ease as if they\nwere running locally. Say we want to read some events from an existing weather-updates topic. We just run Kafka’s\nconsole client, and give it the name of the bootstrap service as its URL.The client connects to the bootstrap server, which tells it what Kafka brokers it should connect to, and it then\nfetches the events from those brokers.All of this works with a simple client running locally on the developer’s machine. No containerizing, no deployment,\nno setup. And the fun part is mirrord operates on the process level, so it doesn’t affect connectivity for the rest\nof your system. This means you can even run multiple applications accessing different clusters or different\nnamespaces at the same time.Access external services through the cluster #When you run an application with mirrord, the connections it initiates will be sent out of a temporary pod on your\nKubernetes cluster. This means your application can connect not only to services in the cluster, but to any\nendpoint in the internet that the cluster has access to. This can be useful if you want your application to\ncommunicate with external services that are configured to only accept communication from your cluster’s IP address,\nor if you want to test your cluster’s network connectivity to external services.In the screenshot below you can see what it looks like when we run curl ifconfig.me (a web service that returns\nthe IP you connect from in its response) with mirrord (IPs partially redacted). When we run the curl command with\nmirrord, it’s sent out of the cluster, so the cluster’s egress IP is returned.Debugging new services with targetless mirrord #Your microservices are deployed on the cluster, so far away. You can see they are there with kubectl. You can even\nport-forward to them. But if you are working on a brand-new microservice that communicates with your existing\nmicroservices, you can’t run it locally, because its requests to the other microservices will fail, especially if\naddressed by cluster-internal domain names. So close, and yet so far. So in order to test your new little service\nwhich barely even does anything, you have to create Kubernetes resources for it, package it in a container, deploy\nit to the cluster, right?Wrong, obviously.Just run it with mirrord, and when it makes requests to services on the cluster, they’ll just work. The DNS\nresolving will be performed on your cluster, and the network traffic will be emitted from within your cluster, so\nyour new little app won’t even notice that it’s running locally and not deployed to the cloud. You can run your app\neither from your IDE (VS Code/IntelliJ-based IDEs) using the mirrord extension, or from the command line, with\nmirrord exec [mirrord-options …] <YOUR-APP> [-- app-args …]. You can iterate through changes in your application,\nrerunning it easily after each little change, and even set breakpoints in your IDE and debug your application,\nwhile it is communicating as if it is running in your cluster.Why is it called “targetless”? #“Targetless” is a new mode for mirrord, which up until now always had to have a target to operate. In the normal\nmode of operation of mirrord, you specify a target container on your cluster (you can specify it by the pod or\neven the deployment, you do not have to know the specific container name). mirrord then spawns an\nagent on the same node as the target, which helps\nyour local program impersonate that container - mirror or steal its incoming traffic, send out network requests\nover it, access the same filesystem and read its environment variables.The main use case for this mode is to debug a new version of an existing application. For example, if you are\nmaking changes to an API endpoint of an existing microservice, you can run the changed version with mirrord, with\nthe existing microservice running in the cloud as a target, and when that service receives requests, they will be\nmirrored (or redirected entirely) to your local application by mirrord. This lets you debug the changed endpoint\nusing traffic from the Kubernetes cluster without deploying a new version after every little change.Can’t I just use kubectl port-forward? #There are some basic tasks that can be achieved with either mirrord or kubectl port-forward in varying degrees of\ncomfort. However, mirrord does something fundamentally different from port-forwarding. mirrord runs an application\nand forwards to the cluster whatever connections it initiates. This means you don’t need to know the ports in\nadvance and configure them before running the application, DNS is resolved in the cluster, UDP is forwarded as well\nas TCP, and connections can also be made to addresses outside the cluster (with the source address of the\ncluster, so if an external service is only open to the cluster’s IP, the application can access it with mirrord).I want to start using it right now! #Look at you, trying out new things, learning every day. Good for you.\nInstalling the mirrord CLI tool is as easy as runningorOr you can install it as a VS Code extension or a\nplugin for IntelliJ-based IDEs.Of course, as mirrord is completely open source you can also\nbuild it from source.Check out the mirrord docs, especially the\nmirrord configuration docs, and the target reference for\nhow to run targetless.\nReach out to us on Discord or GitHub\nfor help, questions, feedback or just to say hi.We used the manifests from Red Hat Developer’s\nKafka in Kubernetes tutorial\nto quickly set up the cluster. ↩︎Share postsign up to our blog post for updates and newsThis field cannot be left blank.Tal ZwickSoftware Engineer @ MetalBear.Share postYou may also like...Approaches in Cloud Development ErgonomicsOctober 31, 2022 by Eyal Bukchin	2023-06-06	{"title":"Easy K8S Connectivity for Local Utils","article":"## Easy K8S Connectivity for Local Utils\\n\\nAre you tired of containerizing and deploying your applications to Kubernetes just to test or debug them? Look no further than mirrord, a CLI tool that allows you to run a program locally on your machine and forward network connections initiated by the program to the cluster. With mirrord's targetless mode, you can even access cluster-internal services that do not have any external IP.\\n\\n### Running utility programs with access to your cluster\\n\\nWhen you run an application with mirrord, DNS resolution and network connection will be done from the cluster, allowing you to run tools locally for configuring or testing your services on the cluster. For example, you can use Kafka's console client to read events from an existing topic without containerizing, deploying, or setting up anything. Mirrord operates on the process level, so it doesn't affect connectivity for the rest of your system. You can even run multiple applications accessing different clusters or different namespaces at the same time.\\n\\n```bash\\necho 'bootstrap.servers=kafka-bootstrap.kafka.svc.cluster.local:9092' > kafka-console-consumer.properties\\nkafka-console-consumer --topic weather-updates --from-beginning --bootstrap-server kafka-bootstrap.kafka.svc.cluster.local:9092\\n```\\n\\n### Access external services through the cluster\\n\\nWhen you run an application with mirrord, the connections it initiates will be sent out of a temporary pod on your Kubernetes cluster. This means your application can connect not only to services in the cluster but to any endpoint on the internet that the cluster has access to. This can be useful if you want your application to communicate with external services that are configured to only accept communication from your cluster's IP address or if you want to test your cluster's network connectivity to external services.\\n\\n```bash\\nmirrord curl ifconfig.me\\n```\\n\\n### Debugging new services with targetless mirrord\\n\\nIf you are working on a brand-new microservice that communicates with your existing microservices, you can't run it locally because its requests to the other microservices will fail, especially if addressed by cluster-internal domain names. With mirrord, you can run your app either from your IDE using the mirrord extension or from the command line, and when it makes requests to services on the cluster, they'll just work. You can iterate through changes in your application, rerunning it easily after each little change, and even set breakpoints in your IDE and debug your application while it is communicating as if it is running in your cluster.\\n\\n### Conclusion\\n\\nMirrord is a powerful tool that simplifies the process of testing and debugging applications in Kubernetes. With its targetless mode, you can run utility programs with access to your cluster, access external services through the cluster, and debug new services without containerizing or deploying anything. Install the mirrord CLI tool, or use it as a VS Code extension or a plugin for IntelliJ-based IDEs, and start using it today!\\n\\nFor more information, check out the [mirrord docs](https://docs.mirrord.dev/), and reach out to us on [Discord](https://discord.gg/mirrord) or [GitHub](https://github.com/mirrord-org) for help, questions, feedback, or just to say hi."}
38	Google finds faster sorting algorithm using deep reinforcement learning	https://www.nature.com/articles/s41586-023-06004-9	failed-too-big	reddit	["Algorithms"]	\N	2023-06-07	\N
\.


--
-- Name: info_id_seq; Type: SEQUENCE SET; Schema: public; Owner: root
--

SELECT pg_catalog.setval('public.info_id_seq', 40, true);


--
-- Name: info info_link_key; Type: CONSTRAINT; Schema: public; Owner: root
--

ALTER TABLE ONLY public.info
    ADD CONSTRAINT info_link_key UNIQUE (link);


--
-- Name: info info_pkey; Type: CONSTRAINT; Schema: public; Owner: root
--

ALTER TABLE ONLY public.info
    ADD CONSTRAINT info_pkey PRIMARY KEY (id);


--
-- PostgreSQL database dump complete
--

